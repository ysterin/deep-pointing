{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_pointing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ysterin/deep-pointing/blob/master/deep_pointing.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "PgmOdt0CjkXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f3b94451-2653-4090-be95-26d8b7c81bcb"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-518a16b2-d0bf-44dc-a929-d099db60b4c7\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-518a16b2-d0bf-44dc-a929-d099db60b4c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ta1.txt to ta1.txt\n",
            "User uploaded file \"ta1.txt\" with length 3062494 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-QRQN2wFuI1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b34bc8bd-da55-4162-f90a-66df1f653640"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "def tensorize(text):\n",
        "    letters = [chr(i) for i in range(1488, 1515)]\n",
        "    pointings_ids = list(range(1456, 1469)) + list(range(1473, 1475))\n",
        "    pointings = [chr(i) for i in pointings_ids]\n",
        "    letters_index = {c:i+1 for i, c in enumerate(letters)}\n",
        "    index_letters = {i+1:c for i, c in enumerate(letters)}\n",
        "    letters_index[' '] = 0\n",
        "    index_letters[0] = ' '\n",
        "    pointings_index = {c:i for i, c in enumerate(pointings)}\n",
        "    index_pointings = {i:c for i, c in enumerate(pointings)}\n",
        "    input_dim = len(letters_index) + 1\n",
        "    output_dim = len(pointings_index) + 1\n",
        "    N = len([c for c in text if c in letters_index or c==' '])\n",
        "    pointing_seqs = re.split(r'\\w| ', text)[1:]\n",
        "    \n",
        "    text_ = ''\n",
        "    for i in range(N):\n",
        "        text_ += unpointed_text[i] + pointing_seqs[i]\n",
        "    assert N == len(unpointed_text)\n",
        "    pointing_seqs = [seq.rjust(3) for seq in pointing_seqs]\n",
        "    \n",
        "    pointings_set_0 = sorted(list({seq[0] for seq in pointing_seqs}))\n",
        "    pointings_set_1 = sorted(list({seq[1] for seq in pointing_seqs}))\n",
        "    pointings_set_2 = sorted(list({seq[2] for seq in pointing_seqs}))\n",
        "\n",
        "    print([ord(p) for p in pointings_set_0])\n",
        "    print([ord(p) for p in pointings_set_1])\n",
        "    print([ord(p) for p in pointings_set_2])\n",
        "\n",
        "    print(Counter([seq[0] for seq in pointing_seqs]))\n",
        "    print(Counter([seq[1] for seq in pointing_seqs]))\n",
        "    print(Counter([seq[2] for seq in pointing_seqs]))\n",
        "\n",
        "    \n",
        "    pointings_index_0 = {c:i for i,c in enumerate(pointings_set_0)}\n",
        "    pointings_index_1 = {c:i for i,c in enumerate(pointings_set_1)}\n",
        "    pointings_index_2 = {c:i for i,c in enumerate(pointings_set_2)}\n",
        "\n",
        "    index_pointings_0 = {i:c for i,c in enumerate(pointings_set_0)}\n",
        "    index_pointings_1 = {i:c for i,c in enumerate(pointings_set_1)}\n",
        "    index_pointings_2 = {i:c for i,c in enumerate(pointings_set_2)}\n",
        "    \n",
        "    output_dim_0 = len(pointings_set_0)\n",
        "    output_dim_1 = len(pointings_set_1)\n",
        "    output_dim_2 = len(pointings_set_2)\n",
        "    \n",
        "    X = np.zeros((N, input_dim), dtype=np.bool)\n",
        "    y0 = np.zeros((N, output_dim_0), dtype=np.bool) \n",
        "    y1 = np.zeros((N, output_dim_1), dtype=np.bool)\n",
        "    y2 = np.zeros((N, output_dim_2), dtype=np.bool)\n",
        "\n",
        "    for i in range(N):\n",
        "        X[i, letters_index[unpointed_text[i]]] = 1\n",
        "        y0[i, pointings_index_0[pointing_seqs[i][0]]] = 1\n",
        "        y1[i, pointings_index_1[pointing_seqs[i][1]]] = 1\n",
        "        y2[i, pointings_index_2[pointing_seqs[i][2]]] = 1\n",
        "    \n",
        "    return X, y0, y1, y2\n",
        "letters = [chr(i) for i in range(1488, 1515)]\n",
        "with open(\"ta1.txt\", 'rb') as file:\n",
        "    text = file.read().decode('utf8')  \n",
        "letters_index = {c:i+1 for i, c in enumerate(letters)}\n",
        "pointings = {chr(i) for i in range(1456, 1468)}\n",
        "pointings.add(chr(65533))\n",
        "#pointings_set = set(pointings)\n",
        "pointing_seqs = re.split(r'\\w| ', text)[1:]\n",
        "unpointed_text = [c for c in text if c in letters_index or c==' ']\n",
        "print(''.join(unpointed_text[294250:294350]))\n",
        "s = set()\n",
        "print(s)\n",
        "# for i, seq in enumerate(pointing_seqs):\n",
        "#   if len(pointings.intersection(set(seq))) > 1:\n",
        "#     #print(i,''.join(unpointed_text[i-15:i+15]))\n",
        "#     if chr(1468) not in pointings.intersection(set(seq)):\n",
        "#        s.add(seq)\n",
        "#        #s.add((sorted(list(pointings.intersection(set(seq))))[0], sorted(list(pointings.intersection(set(seq))))[1]))\n",
        "#        #print(sorted([ord(p) for p in pointings.intersection(set(seq))]))\n",
        "#        #print(i,''.join(unpointed_text[i-4:i+2]))\n",
        "#        if ''.join(unpointed_text[i-4:i+2]) != 'ירושלם':\n",
        "#           print(i,''.join(unpointed_text[i-4:i+4]))\n",
        "#           print(((sorted(list(pointings.intersection(set(seq))))[0], sorted(list(pointings.intersection(set(seq))))[1])))\n",
        "# print([[ord(z) for z in a] for a in s])\n",
        "# s\n",
        "#tensorize(text)\n",
        "X_, y0, y1, y2 = tensorize(text)\n",
        "ys_ = [y0, y1, y2]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "גל ועלה הגבול גי בן הנם אל כתף היבוסי מנגב היא ירושלם ועלה הגבול אל ראש ההר אשר על פני גי הנם ימה אש\n",
            "set()\n",
            "[32, 1468]\n",
            "[32, 1463, 1464, 1468, 1473, 1474]\n",
            "[32, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1467, 1468, 1473, 1474, 65533]\n",
            "Counter({' ': 975676, 'ּ': 3206})\n",
            "Counter({' ': 869124, 'ּ': 77221, 'ׁ': 24754, 'ׂ': 7282, 'ַ': 284, 'ָ': 217})\n",
            "Counter({' ': 439806, 'ָ': 98224, 'ְ': 97005, 'ַ': 76555, 'ִ': 69709, 'ֶ': 50933, 'ֹ': 48219, 'ֵ': 38914, 'ּ': 30607, 'ֲ': 16662, 'ׁ': 5214, 'ֻ': 2978, 'ֱ': 2363, 'ׂ': 1047, 'ֳ': 407, '�': 239})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0drn6umlpLdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "84b5001a-2e97-4ff4-ed8c-7f303623fcdf"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "def tensorize(text):\n",
        "  unpointed_text = [c for c in text if ord(c) in range(1488, 1515) or c==' '][0:]\n",
        "  pointing_seqs = re.split(r'\\w| ', text)[1:]\n",
        "  print(pointing_seqs[:100])\n",
        "  print(unpointed_text[:100])\n",
        "  \n",
        "  assert len(unpointed_text) == len(pointing_seqs)\n",
        "  N = len(unpointed_text)\n",
        "  text_ = ''\n",
        "  for i in range(N):\n",
        "    text_ += unpointed_text[i] + pointing_seqs[i]\n",
        "  assert text_==text\n",
        "  letters_index = { chr(i):i-1487 for i in range(1488, 1515)}\n",
        "  letters_index[' '] = 0\n",
        "  \n",
        "  X = [letters_index[c] for c in unpointed_text]\n",
        "  \n",
        "  pointings = [chr(i) for i in range(1456, 1468)] + [chr(65533)]\n",
        "  diphthongs = [''.join([chr(i) for i in l]) for l in [[1464, 1460], [1463, 1456], [1464, 1456], [1463, 1460]]]\n",
        "  num_pointings = len(pointings)\n",
        "  dagesh = chr(1468)\n",
        "  shin = chr(1473)\n",
        "  sin = chr(1474)\n",
        "  \n",
        "  ys = [np.zeros(N, dtype=np.int16) for _ in range(3)]\n",
        "  \n",
        "  for i, seq in enumerate(pointing_seqs):\n",
        "    if dagesh in seq:\n",
        "      ys[0][i] = 1\n",
        "    if shin in seq:\n",
        "      ys[1][i] = 1\n",
        "    elif sin in seq:\n",
        "      ys[1][i] = 2\n",
        "    \n",
        "    flag = False\n",
        "    for j, diph in enumerate(diphthongs):\n",
        "      if diph in seq:\n",
        "        ys[2][i] = j + num_pointings + 1\n",
        "        flag = True\n",
        "        break\n",
        "    if flag:\n",
        "      continue\n",
        "      \n",
        "    for j, point in enumerate(pointings):\n",
        "      if point in seq:\n",
        "        assert ys[2][i] == 0\n",
        "        ys[2][i] = j + 1\n",
        "  \n",
        "  return X, ys\n",
        "with open(\"ta1.txt\", 'rb') as file:\n",
        "    text = file.read().decode('utf8')\n",
        "X, ys = tensorize(text)\n",
        "print(X[:100])\n",
        "for y in ys:\n",
        "  print(y[:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'ְּ', 'ֵ', '', 'ִׁ', '', '', '', 'ָּ', 'ָ', '', '', 'ֱ', 'ֹ', 'ִ', '', '', '', 'ֵ', '', '', 'ַ', 'ָּׁ', 'ַ', 'ִ', '', '', 'ְ', 'ֵ', '', '', 'ָ', 'ָ', 'ֶ', '', '', 'ְ', 'ָ', 'ָ', 'ֶ', '', '', 'ָ', 'ְ', 'ָ', '', '', 'ֹ', '', 'ּ', '', 'ָ', 'ֹ', '', 'ּ', '', 'ְ', 'ֹ', 'ֶׁ', 'ְ', '', 'ַ', '', '', 'ְּ', 'ֵ', '', '', 'ְ', '', 'ֹ', '', '', 'ְ', '', 'ּ', 'ַ', '', 'ֱ', 'ֹ', 'ִ', '', '', '', 'ְ', 'ַ', 'ֶ', 'ֶ', '', '', 'ַ', '', '', 'ְּ', 'ֵ', '', '', 'ַ', 'ָּ', 'ִ']\n",
            "[' ', 'ב', 'ר', 'א', 'ש', 'י', 'ת', ' ', 'ב', 'ר', 'א', ' ', 'א', 'ל', 'ה', 'י', 'ם', ' ', 'א', 'ת', ' ', 'ה', 'ש', 'מ', 'י', 'ם', ' ', 'ו', 'א', 'ת', ' ', 'ה', 'א', 'ר', 'ץ', ' ', 'ו', 'ה', 'א', 'ר', 'ץ', ' ', 'ה', 'י', 'ת', 'ה', ' ', 'ת', 'ה', 'ו', ' ', 'ו', 'ב', 'ה', 'ו', ' ', 'ו', 'ח', 'ש', 'ך', ' ', 'ע', 'ל', ' ', 'פ', 'נ', 'י', ' ', 'ת', 'ה', 'ו', 'ם', ' ', 'ו', 'ר', 'ו', 'ח', ' ', 'א', 'ל', 'ה', 'י', 'ם', ' ', 'מ', 'ר', 'ח', 'פ', 'ת', ' ', 'ע', 'ל', ' ', 'פ', 'נ', 'י', ' ', 'ה', 'מ', 'י']\n",
            "[0, 2, 25, 1, 26, 10, 27, 0, 2, 25, 1, 0, 1, 13, 5, 10, 14, 0, 1, 27, 0, 5, 26, 15, 10, 14, 0, 6, 1, 27, 0, 5, 1, 25, 22, 0, 6, 5, 1, 25, 22, 0, 5, 10, 27, 5, 0, 27, 5, 6, 0, 6, 2, 5, 6, 0, 6, 8, 26, 11, 0, 19, 13, 0, 21, 17, 10, 0, 27, 5, 6, 14, 0, 6, 25, 6, 8, 0, 1, 13, 5, 10, 14, 0, 15, 25, 8, 21, 27, 0, 19, 13, 0, 21, 17, 10, 0, 5, 15, 10]\n",
            "[0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
            "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[ 0  1  6  0  5  0  0  0  9  9  0  0  2 10  5  0  0  0  6  0  0  8  9  8\n",
            "  5  0  0  1  6  0  0  9  9  7  0  0  1  9  9  7  0  0  9  1  9  0  0 10\n",
            "  0  0  0  9 10  0  0  0  1 10  7  1  0  8  0  0  1  6  0  0  1  0 10  0\n",
            "  0  1  0  0  8  0  2 10  5  0  0  0  1  8  7  7  0  0  8  0  0  1  6  0\n",
            "  0  8  9  5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VoQgLXPVzO6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "43e0666b-a7e4-4dfb-bef1-81bdc539bfc4"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "X = to_categorical(X)\n",
        "ys = [to_categorical(y) for y in ys]\n",
        "X=X_\n",
        "ys = ys_"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FgEBULzZ8R2t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "with open(\"ta1.txt\", 'rb') as file:\n",
        "    text = file.read().decode('utf8')  \n",
        "\n",
        "from collections import defaultdict\n",
        "pointings_unicode = [[32, 1468], \n",
        "             [32, 1463, 1464, 1468, 1473, 1474], \n",
        "             [32, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1473, 1474]]\n",
        "num_pointings = [len(pointing_unicode) for pointing_unicode in pointings_unicode]\n",
        "from itertools import chain\n",
        "\n",
        "letters = [chr(i) for i in range(1488, 1515)]\n",
        "letters_index = {c:i for i, c in enumerate(letters)}\n",
        "index_letters = {i:c for i, c in enumerate(letters)}\n",
        "words = text.split(' ')\n",
        "words_unpointed = [''.join([c for c in word if c in letters_index]) for word in words][1:]\n",
        "num_words = len(words_unpointed)\n",
        "print(len(''.join(words_unpointed))/num_words)\n",
        "words_pointings = [re.split(r'\\w', word)[1:] for word in words][1:]\n",
        "words_pointings = [[seq.rjust(3) for seq in pointings] for pointings in words_pointings]\n",
        "words_pointings_flat = list(chain.from_iterable(words_pointings))\n",
        "words_pointings_dicts = [defaultdict(list) for i in range(3)]\n",
        "for i in range(num_words):\n",
        "  for l in range(3):\n",
        "    words_pointings_dicts[l][words_unpointed[i]] += [\"\".join([p[l] for p in words_pointings[i]])]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_75pyaN9Qzk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "num_unique_words = len(words_pointings_dicts[2])\n",
        "for l in range(3):\n",
        "  sum = 0\n",
        "  for word, pointings in words_pointings_dicts[l].items():\n",
        "    try:\n",
        "      sum += Counter(pointings).most_common(1)[0][1]\n",
        "    except IndexError:\n",
        "      print((word, pointings))\n",
        "  print(l)\n",
        "  print(\"sum: \", sum)\n",
        "  print(\"avarage: \", sum/num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UthJS7owuksn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "pointings_unicode = [[32, 1468], \n",
        "             [32, 1463, 1464, 1468, 1473, 1474], \n",
        "             [32, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1473, 1474]]\n",
        "num_pointings = [len(pointing_unicode) for pointing_unicode in pointings_unicode]\n",
        "from itertools import chain\n",
        "import re\n",
        "import numpy as np\n",
        "with open(\"ta1.txt\", 'rb') as file:\n",
        "    text = file.read().decode('utf8')  \n",
        "def tensorize_words(text, pad_zeros=True):\n",
        "  letters = [chr(i) for i in range(1488, 1515)]\n",
        "  if not pad_zeros:\n",
        "    letters = [' '] + letters\n",
        "  letters_index = {c:i for i, c in enumerate(letters)}\n",
        "  index_letters = {i:c for i, c in enumerate(letters)}\n",
        "  words = text.split(' ')\n",
        "  words_unpointed = [''.join([c for c in word if c in letters_index]) for word in words][1:]\n",
        "  num_words = len(words_unpointed)\n",
        "  words_pointings = [re.split(r'\\w', word)[1:] for word in words][1:]\n",
        "  words_pointings = [[seq.rjust(3) for seq in pointings] for pointings in words_pointings]\n",
        "  words_pointings_flat = list(chain.from_iterable(words_pointings))\n",
        "#   words_pointings_dicts = [defaultdict(list) for i in range(3)]\n",
        "#   for i in range(num_words):\n",
        "#     for l in range(3):\n",
        "#       words_pointings_dicts[l][words_unpointed[i]] += [\"\".join([p[l] for p in words_pointings[i]])]\n",
        "  #print(words_pointings[:2])\n",
        "  #print(words_unpointed[:2])\n",
        "  max_word_len = max([len(w) for w in words_unpointed])\n",
        "  num_letters = len(letters)\n",
        "  pointings_unic = [sorted(list({ord(p[l]) for p in words_pointings_flat})) for l in range(3)]\n",
        "  #print(pointings_unic)\n",
        "  #print(pointings_unicode)\n",
        "  num_pointings = [len(pointings) for pointings in pointings_unicode]\n",
        "  index_pointings = [{ i:chr(pointing_unicode[i]) for i in range(num_pointings[j])} for j, pointing_unicode in enumerate(pointings_unicode)]\n",
        "  pointings_index = [{ chr(pointing_unicode[i]):i for i in range(num_pointings[j])} for j, pointing_unicode in enumerate(pointings_unicode)]\n",
        "  num_words = len(words_unpointed)\n",
        "  \n",
        "  X = np.zeros((num_words, max_word_len, num_letters), dtype=np.bool)\n",
        "  ys = [np.zeros((num_words, max_word_len, num_pointings[i]), dtype=np.bool) for i in range(3)]\n",
        "  for i, word in enumerate(words_unpointed):\n",
        "    for j, letter in enumerate(word):\n",
        "      k = letters_index[letter]\n",
        "      X[i, j, k] = 1\n",
        "      for l in range(3):\n",
        "        try:\n",
        "          k = pointings_index[l][words_pointings[i][j][l]]\n",
        "          ys[l][i, j, k] = 1\n",
        "        except KeyError:\n",
        "          print(pointings_index[l])\n",
        "          print((l, words_pointings[i]))\n",
        "    for j in range(len(word), max_word_len):\n",
        "      if not pad_zeros:\n",
        "        X[i, j, 0] = 1\n",
        "      for l in range(3):\n",
        "        ys[l][i, j, 0] = 1\n",
        "  return X, ys\n",
        "\n",
        "X, ys = tensorize_words(text)\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DblFP-aHuBiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import Regularizer, l2, l1\n",
        "from keras.layers import Dense, Activation, Input, Dropout, GaussianNoise, concatenate, Reshape, Input\n",
        "from keras.layers import LSTM, SimpleRNN, Bidirectional, GRU, CuDNNLSTM, CuDNNGRU\n",
        "from keras.optimizers import RMSprop, Nadam, SGD\n",
        "from keras.models import Model\n",
        "from keras.layers import ActivityRegularization, Masking, TimeDistributed, Concatenate, Multiply\n",
        "from keras.callbacks import TerminateOnNaN\n",
        "if device_name == '/device:GPU:0':\n",
        "  lstm = CuDNNLSTM \n",
        "else:\n",
        "  lstm = LSTM\n",
        "lstm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yU2v6bkGlj25",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oeBoLS8zmLUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_ = X\n",
        "#@title\n",
        "l = 2\n",
        "input_dims = (X_.shape[1], X_.shape[2])\n",
        "output_dims = (ys[l].shape[1], ys[l].shape[2])\n",
        "input_reshape_dim = X_.shape[1]*X_.shape[2]\n",
        "model = Sequential()\n",
        "#model.add(Input(shape=input_dims))\n",
        "model.add(Reshape(target_shape=(input_reshape_dim, ), input_shape=input_dims))\n",
        "\n",
        "#attention\n",
        "#model.add(Dense(input_reshape_dim, activation='softmax', name='attention_probs'))\n",
        "#model.add(Multiply())\n",
        "\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(Dropout(0.2))\n",
        "# model.add(Dense(256, activation='tanh'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(256, activation='tanh'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(256, activation='tanh'))\n",
        "# model.add(Dropout(0.2))\n",
        "#model.add(Dense(48, input_shape = (input_dim,), activation='relu'))\n",
        "model.add(Dense(ys[l].shape[1]*ys[l].shape[2]))\n",
        "model.add(Reshape(target_shape=output_dims))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "optimizer = SGD(lr=0.005, momentum=0.99, clipnorm=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "model.fit(X_, ys[l], validation_split=0.333, batch_size=32, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UStBCK7ccjNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Permute\n",
        "l = 2\n",
        "num_words, word_len, num_letters = X.shape\n",
        "output_dim = ys[l].shape[2]\n",
        "input_reshape_dim = word_len*num_letters\n",
        "\n",
        "inputs = Input(shape=(word_len, num_letters))\n",
        "\n",
        "#Attention:\n",
        "permute = Permute((2,1), name='petmute_inputs')(inputs)\n",
        "attention_probs = Dense(word_len, activation='softmax', name='attantion_probs_d2')(permute)\n",
        "attention_probs_d2 = Permute((2,1), name='permute_attention_probs')(attention_probs)\n",
        "\n",
        "attention_probs_d1 = Dense(num_letters, activation='softmax', name='attention_probs_d1')(inputs)\n",
        "\n",
        "merge = Multiply(name='merge_attention')([attention_probs_d1 ,attention_probs_d2, inputs])\n",
        "#attention\n",
        "\n",
        "reshape = Reshape(target_shape=(word_len*num_letters,), name='input_reshape')(inputs)\n",
        "\n",
        "dense = Dense(128, activation='tanh', name='dense1')(reshape)\n",
        "dense = Dense(output_dim*word_len, name='dense_final')(dense)\n",
        "\n",
        "reshape = Reshape(target_shape=(word_len, output_dim), name='output_reshape')(dense)\n",
        "outputs = Activation('softmax', name='outputs')(reshape)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "print(model.summary())\n",
        "optimizer = RMSprop(lr=0.005)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, ys[2], validation_split=0.25, batch_size=32, epochs=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fW2V7b8L7Mu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **svm** and logistic regression for reference"
      ]
    },
    {
      "metadata": {
        "id": "0jLzoAFoxw8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "l = 2\n",
        "s = 0\n",
        "for k in range(11):\n",
        "  N = num_words\n",
        "  num_words, word_len, output_dim = ys[l].shape\n",
        "  num_words, word_len, num_letters = X.shape\n",
        "  y = ys[2][:,k,:].reshape((num_words, output_dim))\n",
        "  y_ = np.argmax(y, axis=-1)\n",
        "  X_ = X.reshape((num_words, word_len*num_letters))\n",
        "\n",
        "  clf = LogisticRegression()\n",
        "\n",
        "  try:\n",
        "    m = np.mean(cross_validate(clf, X_[:N,:], y_[:N], cv=2)[\"test_score\"])\n",
        "  except ValueError:\n",
        "    m = 1.0\n",
        "  print(m)\n",
        "  \n",
        "  s += m\n",
        "  \n",
        "print(s)\n",
        "print(s/11)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HAbOyjf1CAht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_predict = np.argmax(model.predict(X), axis=-1)\n",
        "y = np.argmax(ys[l], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsdeqDnBEG1D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(y_predict[:10,:])\n",
        "print(y[:10,:])\n",
        "print(np.sum(np.equal(y_predict, y))/(y.shape[0]*y.shape[1]))\n",
        "11/(len(''.join(words_unpointed))/num_words)\n",
        "print(y.shape[0])\n",
        "np.sum([np.array_equal(y[i,:], y_predict[i,:]) for i in range(y.shape[0])])/y.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oeB0ulXXzG9P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_len = 10\n",
        "def create_sequences(X, ys, seq_len=seq_len):\n",
        "  num_words, word_len, num_letters = X.shape\n",
        "  num_pointings = [y.shape[2] for y in ys]\n",
        "  num_sequences = num_words - seq_len + 1\n",
        "  X_sequences = np.zeros((num_sequences, seq_len, word_len, num_letters), dtype=np.bool)\n",
        "  ys_sequences = [np.zeros((num_sequences, seq_len, word_len, num_pointings[i]), dtype=np.bool) for i in range(3)]\n",
        "  \n",
        "  for i in range(num_sequences):\n",
        "    X_sequences[i, :, :, :] = X[i:i+seq_len, :, :]\n",
        "    for l in range(3):\n",
        "      ys_sequences[l][i, :, :, :] = ys[l][i:i+seq_len, :, :]\n",
        "      \n",
        "  return X_sequences, ys_sequences\n",
        "\n",
        "N = X.shape[0]\n",
        "val_idx = int(0.6*N)\n",
        "test_idx = int(0.8*N)\n",
        "\n",
        "X_train_seq, ys_train_seq = create_sequences(X[:val_idx,:,:], [y[:val_idx,:,:] for y in ys])\n",
        "X_val_seq, ys_val_seq = create_sequences(X[val_idx:test_idx,:,:], [y[val_idx:test_idx,:,:] for y in ys])\n",
        "X_test_seq, ys_test_seq = create_sequences(X[test_idx:,:,:], [y[test_idx:,:,:] for y in ys])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LjrnrTKPvmlQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l = 2\n",
        "do_attention = True\n",
        "_, seq_len, word_len, num_letters = X_train_seq.shape\n",
        "output_dim = num_pointings[l]\n",
        "\n",
        "inputs = Input(shape=(seq_len, word_len, num_letters), name='input')\n",
        "reshape = Reshape(target_shape=(seq_len*word_len, num_letters), name='reshape_input')(inputs)\n",
        "\n",
        "#attention\n",
        "permute = Permute((2,1), name='permute_input')(reshape)\n",
        "attention_probs = Dense(seq_len*word_len, name='attention_probs', kernel_initializer='ones')(permute)\n",
        "merge = Multiply(name='merge')([permute, attention_probs])\n",
        "permute = Permute((2,1), name='parmute_back')(merge)\n",
        "\n",
        "rnn_input = permute if do_attention else reshape\n",
        "rnn = Bidirectional(lstm(32, return_sequences=True), name='lstm1')(permute)\n",
        "dense = Dense(output_dim, name='dense_final')(rnn)\n",
        "reshape = Reshape(target_shape=(seq_len, word_len, output_dim), name='reshape_output')(dense)\n",
        "outputs = Activation('softmax', name='output')(reshape)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "print(model.summary())\n",
        "\n",
        "optimizer = 'rmsprop'\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_seq, ys_train_seq[l], validation_data=(X_val_seq, ys_val_seq[l]), batch_size=32, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TC8k_TewXSJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import ActivityRegularization, Masking, CuDNNGRU, TimeDistributed\n",
        "l = 2\n",
        "_, seq_len, word_len, num_letters = X_train_seq.shape\n",
        "print((seq_len, word_len*num_letters))\n",
        "output_dim = num_pointings[l]\n",
        "model = Sequential()\n",
        "#model.add(Reshape(target_shape=(seq_len*word_len, num_letters), input_shape=(seq_len, word_len, num_letters)))\n",
        "#model.add(Masking(mask_value=0.0))\n",
        "model.add(Reshape(target_shape=(seq_len, -1), input_shape=(seq_len, word_len, num_letters)))\n",
        "\n",
        "model.add(TimeDistributed(Dense(128, activation='tanh')))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(ActivityRegularization(l2=0.01))\n",
        "model.add(Bidirectional(lstm(64, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(ActivityRegularization(l2=0.01))\n",
        "model.add(Dense(output_dim*word_len))\n",
        "\n",
        "model.add(Reshape(target_shape = (seq_len, word_len, output_dim)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "optimizer = Nadam(lr=0.002, clipnorm=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_seq, ys_train_seq[l], validation_data=(X_val_seq, ys_val_seq[l]), batch_size=32, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5q1RLXIvG3N5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "kUNMQflFxqOo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "models = [model]\n",
        "num_seqs, seq_len, word_len, num_letters = X_val_seq.shape\n",
        "y_predict = np.argmax(models[0].predict(X_val_seq, batch_size=32), axis=-1)\n",
        "y_true = np.argmax(ys_val_seq[l], axis=-1)\n",
        "print(np.sum(np.equal(y_predict, y_true))/(num_seqs*seq_len*word_len))\n",
        "for k in range(seq_len):\n",
        "  print(k, \": \", np.sum([np.array_equal(y_predict[j, k, :], y_true[j, k, :]) for j in range(num_seqs)])/num_seqs)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGIB20OO2wTa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, ys_train = X[:val_idx,:,:], [y[:val_idx,:,:] for y in ys]\n",
        "X_val, ys_val = X[val_idx:,:,:], [y[val_idx:,:,:] for y in ys]\n",
        "from keras.layers import Concatenate\n",
        "_, seq_len, word_len, num_letters = X_train_seq.shape\n",
        "l = 2\n",
        "output_dim = num_pointings[l]\n",
        "FC_input = Input(shape=(word_len, num_letters), name='input')\n",
        "\n",
        "char_layer1 = Dense(16, activation='tanh')\n",
        "distributed_layer1 = TimeDistributed(char_layer1)\n",
        "distributed1 = distributed_layer1(FC_input)\n",
        "\n",
        "char_layer2 = Dense(8, activation='tanh')\n",
        "distributed_layer2 = TimeDistributed(char_layer2)\n",
        "concat1 = Concatenate()([FC_input, distributed1])\n",
        "distributed2 = distributed_layer2(concat1)\n",
        "\n",
        "char_layer3 = Dense(4, activation='tanh')\n",
        "distributed_layer3 = TimeDistributed(char_layer3)\n",
        "concat2 = Concatenate()([FC_input, distributed1, distributed2])\n",
        "distributed3 = distributed_layer3(concat2)\n",
        "\n",
        "FC_reshape_distributed = Reshape(target_shape=(8*word_len,))(distributed2)\n",
        "FC_reshape = Reshape(target_shape=(word_len*num_letters,))(FC_input)\n",
        "\n",
        "dense_layer1 = Dense(32, activation='tanh')\n",
        "FC_dense1 = dense_layer1(FC_reshape_distributed)\n",
        "#FC_dense1 = Dropout(0.2)(FC_dense1)\n",
        "\n",
        "dense_layer2 = Dense(32, activation='tanh')\n",
        "FC_dense2 = dense_layer2(FC_dense1)\n",
        "#FC_dense1 = Dropout(0.2)(FC_dense1)\n",
        "\n",
        "dense_layer3 = Dense(32, activation='tanh')\n",
        "FC_dense3 = dense_layer3(FC_dense2)\n",
        "#FC_dense1 = Dropout(0.2)(FC_dense1)\n",
        "\n",
        "FC_dense_final = Dense(num_pointings[l]*word_len)(FC_dense1)\n",
        "FC_reshape2 = Reshape(target_shape=(word_len, output_dim))(FC_dense_final)\n",
        "FC_softmax = Activation('softmax')(FC_reshape2)\n",
        "\n",
        "FC_model = Model(inputs=FC_input, outputs=FC_softmax)\n",
        "\n",
        "optimizer = Nadam(lr=0.001)\n",
        "FC_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(FC_model.summary())\n",
        "histories[FC_model] = FC_model.fit([X_train], [ys_train[l]], validation_split=0.2, batch_size=32, epochs=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "18OaS07e8Ix0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "histories = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jzVM-iWY0B0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X_train_seq, ys_train_seq = X_train, ys_train\n",
        "# X_val_seq, ys_val_seq = X_val, ys_val\n",
        "# X_test_seq, ys_test_seq = X_test, ys_test\n",
        "\n",
        "X_train, ys_train = X[:val_idx,:,:], [y[:val_idx,:,:] for y in ys]\n",
        "X_val, ys_val = X[val_idx:,:,:], [y[val_idx:,:,:] for y in ys]\n",
        "\n",
        "_, seq_len, word_len, num_letters = X_train_seq.shape\n",
        "l = 2\n",
        "output_dim = num_pointings[l]\n",
        "FC_input = Input(shape=(word_len, num_letters))\n",
        "\n",
        "#distributed_layer = TimeDistributed(Dense(8, activation='tanh'))\n",
        "# distributed_layer = TimeDistributed(char_layer)\n",
        "# distributed = distributed_layer(FC_input)\n",
        "# FC_reshape_distributed = Reshape(target_shape=(-1,))(distributed)\n",
        "FC_reshape1 = Reshape(target_shape=(word_len*num_letters,))(FC_input)\n",
        "\n",
        "dense_layer1 = Dense(128, activation='tanh')\n",
        "FC_dense1 = dense_layer1(FC_reshape1)\n",
        "FC_dense1 = Dropout(0.2)(FC_dense1)\n",
        "\n",
        "dense_layer2 = Dense(128, activation='tanh')\n",
        "FC_dense2 = dense_layer2(FC_dense1)\n",
        "FC_dense2 = Dropout(0.2)(FC_dense2)\n",
        "\n",
        "\n",
        "dense_layer3 = Dense(num_pointings[l]*word_len)\n",
        "FC_dense3 = dense_layer3(FC_dense1)\n",
        "FC_reshape2 = Reshape(target_shape=(word_len, output_dim))(FC_dense3)\n",
        "FC_softmax = Activation('softmax')(FC_reshape2)\n",
        "\n",
        "FC_model = Model(inputs=FC_input, outputs=FC_softmax)\n",
        "\n",
        "optimizer = Nadam(lr=0.001)\n",
        "FC_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(FC_model.summary())\n",
        "histories[FC_model] = FC_model.fit([X_train], [ys_train[l]], validation_split=0.2, batch_size=32, epochs=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQYTn3U53uoR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dense_layer = dense_layer1\n",
        "#dense_layer.trainable = False\n",
        "from keras.layers import Concatenate\n",
        "seq_input = Input(shape=(seq_len, word_len, num_letters))\n",
        "\n",
        "char_lstm_dis = TimeDistributed(Bidirectional(lstm(64, return_sequences=True)))(seq_input)\n",
        "char_lstm_dis = Dropout(0.3)(char_lstm_dis)\n",
        "char_lstm_dis = TimeDistributed(Bidirectional(lstm(64, return_sequences=True), merge_mode='concat'))(char_lstm_dis)\n",
        "char_lstm_dis = Dropout(0.3)(char_lstm_dis)\n",
        "#char_lstm_dis = TimeDistributed(Activation('softmax'))(char_lstm_dis)\n",
        "char_lstm_dis = TimeDistributed(Dense(output_dim, activation='softmax'))(char_lstm_dis)\n",
        "\n",
        "char_reshape = Reshape(target_shape=(seq_len*word_len, num_letters))(seq_input)\n",
        "char_lstm = Bidirectional(lstm(64, return_sequences=True))(char_reshape)\n",
        "#char_lstm = ActivityRegularization(l2=0.00)(char_lstm)\n",
        "char_lstm = Dropout(0.3)(char_lstm)\n",
        "\n",
        "char_lstm = Bidirectional(lstm(32, return_sequences=True))(char_lstm)\n",
        "#char_lstm = ActivityRegularization(l2=0.00)(char_lstm)\n",
        "char_lstm = Dropout(0.3)(char_lstm)\n",
        "\n",
        "char_lstm2 = Bidirectional(lstm(64, return_sequences=True))(char_lstm)\n",
        "char_dense_final = TimeDistributed(Dense(output_dim))(char_lstm)\n",
        "char_dense_final = Dropout(0.3)(char_dense_final)\n",
        "char_reshape = Reshape(target_shape=(seq_len, word_len, output_dim))(char_dense_final)\n",
        "char_output = Activation('softmax', name='char')(char_reshape)\n",
        "\n",
        "char_lstm = TimeDistributed(Dense(16, activation='tanh'))(char_lstm)\n",
        "char_lstm = TimeDistributed(Dropout(0.3))(char_lstm)\n",
        "char_lstm = Reshape(target_shape=(seq_len, -1))(char_lstm)\n",
        "\n",
        "#distributed_layer = TimeDistributed(char_layer)\n",
        "#char_dense = distributed_layer(char_reshape)\n",
        "\n",
        "seq_reshape1 = Reshape(target_shape=(seq_len, -1))(seq_input)\n",
        "\n",
        "#seq_dense1 = TimeDistributed(Dense(128, activation='tanh'))(seq_reshape1)\n",
        "seq_dense1 = dense_layer1(seq_reshape1)\n",
        "seq_dense1 = Dropout(0.2)(seq_dense1)\n",
        "\n",
        "seq_dense2 = dense_layer2(seq_dense1)\n",
        "seq_dense2 = Dropout(0.2)(seq_dense2)\n",
        "\n",
        "seq_lstm1_layer = Bidirectional(lstm(64, return_sequences=True))\n",
        "seq_lstm1 = seq_lstm1_layer(seq_dense1)\n",
        "seq_lstm1 = Dropout(0.2)(seq_lstm1)\n",
        "#seq_lstm1 = ActivityRegularization(l2=0.00)(seq_lstm1)\n",
        "#seq_lstm2 = Bidirectional(lstm(64, return_sequences=True))(seq_lstm1)\n",
        "#seq_lstm2 = Dropout(0.2)(seq_lstm2)\n",
        "\n",
        "#seq_concat = Dropout(0.0)(Concatenate()([seq_lstm1, char_lstm]))\n",
        "\n",
        "seq_dense2 = Dense(word_len*output_dim)(char_lstm)\n",
        "seq_dense2 = Dropout(0.2)(seq_dense2)\n",
        "#seq_dense2 = ActivityRegularization(l2=0.00)(seq_dense2)\n",
        "seq_reshape2 = Reshape(target_shape=(seq_len, word_len, output_dim))(seq_dense2)\n",
        "seq_softmax = Activation('softmax', name='seq')(seq_reshape2)\n",
        "\n",
        "seq_model = Model(inputs=seq_input, outputs=char_lstm_dis)\n",
        "#print(histories[seq_model])\n",
        "optimizer = RMSprop(lr=0.005, clipnorm=1.0)\n",
        "seq_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(seq_model.summary())\n",
        "histories[seq_model] = seq_model.fit([X_train_seq], [ys_train_seq[l]], validation_data=([X_val_seq], [ys_val_seq[l]]), batch_size=32, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "siJBOh6nED0Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "histories[seq_model] = seq_model.fit([X_train_seq], [ys_train_seq[l]], validation_data=([X_val_seq], [ys_val_seq[l]]), batch_size=32, epochs=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7oln6czQAzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "histories[seq_model] = seq_model.fit([X_train_seq], [ys_train_seq[l]], validation_data=([X_val_seq], [ys_val_seq[l]]), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RqxyNDWpt-1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ***char level***"
      ]
    },
    {
      "metadata": {
        "id": "t68C3ruOoEBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "113e4d1a-07d7-4f0c-ed90-0950fd85dfa6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "maxlen = 60\n",
        "step = 15\n",
        "def batch(data, maxlen=maxlen, step=step):\n",
        "  n = (len(data)-maxlen)//step - 1\n",
        "  return np.stack([data[i*step:i*step+maxlen,:] for i in range(n)])\n",
        "def multi_batch(datas, maxlen=maxlen, step=step):\n",
        "  return np.concatenate([batch(data, maxlen, step) for data in datas])\n",
        "\n",
        "from random import shuffle\n",
        "N = X.shape[0]\n",
        "N_subs = 100\n",
        "sublen = N//N_subs\n",
        "indices = list(range(N))\n",
        "subs_ids = [list(range(i*sublen, (i+1)*sublen)) for i in range(N_subs)]\n",
        "shuffle(subs_ids)\n",
        "\n",
        "val_subs_idx = int(0.6*N_subs)\n",
        "test_subs_idx = int(0.8*N_subs)\n",
        "\n",
        "#subs_indices = [indices[sub] for sub in subs_ids]\n",
        "subs_x = [X[sub] for sub in subs_ids]\n",
        "subs_ys = [[y[sub] for sub in subs_ids] for y in ys]\n",
        "\n",
        "#subs_indices_train, subs_indices_val, subs_indices_test = subs_indices[:val_subs_idx], subs_indices[val_subs_idx:test_subs_idx], subs_indices[test_subs_idx:]\n",
        "subs_x_train, subs_x_val, subs_x_test = subs_x[:val_subs_idx], subs_x[val_subs_idx:test_subs_idx], subs_x[test_subs_idx:]\n",
        "subs_ys_train, subs_ys_val, subs_ys_test = [sy[:val_subs_idx] for sy in subs_ys], [sy[val_subs_idx:test_subs_idx] for sy in subs_ys], [sy[test_subs_idx:] for sy in subs_ys]\n",
        "\n",
        "x_train, x_val, x_test = multi_batch(subs_x_train), multi_batch(subs_x_val), multi_batch(subs_x_test)\n",
        "ys_train, ys_val, ys_test = [multi_batch(sy) for sy in subs_ys_train], [multi_batch(sy) for sy in subs_ys_val], [multi_batch(sy) for sy in subs_ys_test]\n",
        "print(x_train.shape)\n",
        "\n",
        "# train_ids, val_ids, test_ids = np.concatenate(subs[:val_subs_idx]), np.concatenate(subs[val_subs_idx:test_subs_idx]), np.concatenate(subs[test_subs_idx:])\n",
        "# #n_train, n_val, n_test = train_ids.shape[0], val_ids.shape[0], test_ids.shape[0]\n",
        "# #train_ids, val_ids, test_ids = train_ids[np.random.permutation(n_train)], val_ids[np.random.permutation(n_val)], test_ids[np.random.permutation(n_test)]\n",
        "\n",
        "# val_idx = int(0.6*N)\n",
        "# test_idx = int(0.8*N)\n",
        "\n",
        "# x_train_flat, x_val_flat, x_test_flat = X[train_ids], X[val_ids], X[test_ids]\n",
        "# ys_train_flat, ys_val_flat, ys_test_flat = [y[train_ids] for y in ys], [y[val_ids] for y in ys], [y[test_ids] for y in ys]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(38820, 60, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Y3ZIW3WpVma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "1daba976-d2e2-433e-de12-ad17b90431fb"
      },
      "cell_type": "code",
      "source": [
        "# delete parts of words from the beginning and the end\n",
        "def clean_edges(x, ys):\n",
        "  N = x.shape[0]\n",
        "  x_dim = x.shape[2]\n",
        "  ys_dim = [y.shape[2] for y in ys]\n",
        "  for i in range(N):\n",
        "    j = 0\n",
        "    while not x[i, j, 0]:\n",
        "      x[i, j, :] = np.zeros(x_dim, dtype=np.bool)\n",
        "      x[i, j, 0] = True\n",
        "      for k in range(3):\n",
        "        ys[k][i, j, :] = np.zeros(ys_dim[k], dtype=np.bool)\n",
        "        ys[k][i, j, 0] = True\n",
        "      j += 1\n",
        "    j = -1\n",
        "    while not x[i, j, 0]:\n",
        "      x[i, j, :] = np.zeros(x_dim, dtype=np.bool)\n",
        "      x[i, j, 0] = True\n",
        "      for k in range(3):\n",
        "        ys[k][i, j, :] = np.zeros(ys_dim[k], dtype=np.bool)\n",
        "        ys[k][i, j, 0] = True\n",
        "      j -= 1\n",
        "\n",
        "clean_edges(x_train, ys_train)\n",
        "clean_edges(x_val, ys_val)\n",
        "clean_edges(x_test, ys_test)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UtjK-fJ4ogGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6814051c-77bd-4464-d37a-bf6c67a18a86"
      },
      "cell_type": "code",
      "source": [
        "# maxlen = 240\n",
        "# step = 5\n",
        "# def batch(data, maxlen=maxlen, step=step):\n",
        "#   n = (len(data)-maxlen)//step - 1\n",
        "#   return np.stack([data[i*step:i*step+maxlen,:] for i in range(n)])\n",
        "\n",
        "# x_train, x_val, x_test = batch(x_train_flat), batch(x_val_flat), batch(x_test_flat)\n",
        "# ys_train, ys_val, ys_test = [batch(y) for y in ys_train_flat], [batch(y) for y in ys_val_flat], [batch(y) for y in ys_test_flat]\n",
        "print([y.shape for y in ys_train])\n",
        "n_train, n_val, n_test = x_train.shape[0], x_val.shape[0], x_test.shape[0]\n",
        "train_permute, val_permute, test_permute = np.random.permutation(n_train), np.random.permutation(n_val), np.random.permutation(n_test)\n",
        "\n",
        "x_train, x_val, x_test = x_train[train_permute], x_val[val_permute], x_test[test_permute]\n",
        "ys_train = [y[train_permute] for y in ys_train]\n",
        "ys_val = [y[val_permute] for y in ys_val]\n",
        "ys_test = [y[test_permute] for y in ys_test]\n",
        "\n",
        "N = X.shape[0]\n",
        "input_dim = X.shape[1]\n",
        "output_dims = [ys[i].shape[1] for i in range(3)]\n",
        "# inputs = []\n",
        "# outputs = [[],[],[]]\n",
        "\n",
        "# for i in range((N-maxlen)//step - 1):\n",
        "#   inputs += [X[i*step:i*step+maxlen,:]]\n",
        "#   for j in range(3):\n",
        "#     outputs[j] += [ys[j][i*step:i*step+maxlen,:]]\n",
        "    \n",
        "\n",
        " "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(38820, 60, 2), (38820, 60, 3), (38820, 60, 18)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zAJH29k2nLzX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "0cb564fb-3fd4-4bcf-962b-5136b7cf31a2"
      },
      "cell_type": "code",
      "source": [
        "def batch(data, maxlen=maxlen, step=step):\n",
        "  n = (len(data)-maxlen)//step - 1\n",
        "  return np.stack([data[i*step:i*step+maxlen,:] for i in range(n)])\n",
        "\n",
        "N = X.shape[0]\n",
        "val_idx = int(0.6*N)\n",
        "test_idx = int(0.8*N)\n",
        "\n",
        "x_train = batch(X[:val_idx, :])\n",
        "x_val = batch(X[val_idx:test_idx, :])\n",
        "x_test = batch(X[test_idx:, :])\n",
        "\n",
        "ys_train = [batch(y[:val_idx, :]) for y in ys]\n",
        "ys_val = [batch(y[val_idx:test_idx, :]) for y in ys]\n",
        "ys_test = [batch(y[test_idx:, :]) for y in ys]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1f17D4TktC6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2549c2c0-dec4-4bdd-c383-559c60d4d6c0"
      },
      "cell_type": "code",
      "source": [
        "print([y.shape for y in ys_train])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(39150, 60, 2), (39150, 60, 6), (39150, 60, 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jH9a8iCeo6BJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "r210DGLZqMdF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# inputs = np.asarray(inputs)\n",
        "# outputs = [np.asarray(output) for output in outputs]\n",
        "# print(inputs.shape)\n",
        "# print([output.shape for output in outputs])\n",
        "\n",
        "\n",
        "# shuffle_ids = np.random.permutation(inputs.shape[0])\n",
        "\n",
        "\n",
        "# inputs = inputs[shuffle_ids]\n",
        "# outputs = [output[shuffle_ids] for output in outputs]\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# input_train, input_test = train_test_split(inputs, shuffle = False)\n",
        "# outputs_train = [train_test_split(output, shuffle = False)[0] for output in outputs]\n",
        "# outputs_test = [train_test_split(output, shuffle = False)[1] for output in outputs]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ugb9EB7Sq3fl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ac7f24a4-ef92-4596-ec71-70267c0a30ef"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.regularizers import Regularizer, l2, l1\n",
        "from keras.layers import Dense, Activation, Input, Dropout, GaussianNoise, concatenate\n",
        "from keras.layers import LSTM, SimpleRNN, Bidirectional, GRU, CuDNNLSTM\n",
        "from keras.layers import Multiply, Lambda, Permute, RepeatVector, TimeDistributed, Reshape\n",
        "from keras.optimizers import RMSprop, Nadam\n",
        "from keras.models import Model\n",
        "from keras.callbacks import TerminateOnNaN\n",
        "from keras import backend as K"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PpEC1rDEEOkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "213946bb-ab2b-449b-9a19-fb9b6fe69380"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  lstm = LSTM\n",
        "else:\n",
        "  lstm = CuDNNLSTM\n",
        "print(lstm)\n",
        "def layer (units, dropout=0.2):\n",
        "  return lambda x: Dropout(dropout)(Bidirectional(lstm(units, return_sequences=True))(Dropout(dropout)(x)))\n",
        "def layer_ (units, dropout=0.2):\n",
        "  return Bidirectional(lstm(units, return_sequences=True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYvW_aWo4lY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "4456f6b2-fbcb-4f54-9362-fac66b159f25"
      },
      "cell_type": "code",
      "source": [
        "def attention_mechanism(inputs, single_attention_vector=False):\n",
        "  input_size = int(inputs.shape[-1])\n",
        "  step_size = int(inputs.shape[-2])\n",
        "  permute = Permute((2,1), name='permute_attention_inputs')(inputs)\n",
        "  attention_probs = Dense(step_size, activation='softmax', name='attention_probs', kernel_initializer='identity')(permute)\n",
        "  if single_attention_vector:\n",
        "    attention_probs = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(attention_probs)\n",
        "    attention_probs = RepeatVector(input_size)(attention_probs)\n",
        "  merge = Multiply(name='merge')([permute, attention_probs])\n",
        "  attention_output = Permute((2,1), name='permute_attention_output')(merge)\n",
        "  return attention_output"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-bj1cxx9huu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "56ed43c2-fc24-4ea3-c6b9-afd03c4621c3"
      },
      "cell_type": "code",
      "source": [
        "N = X.shape[0]\n",
        "for y in ys:\n",
        "  print(np.sum(y, axis=0)/N)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8865706 0.1134294]\n",
            "[0.9608768  0.03061452 0.00850869]\n",
            "[4.86957570e-01 9.90936599e-02 2.41397840e-03 1.70214592e-02\n",
            " 4.15780452e-04 7.07051514e-02 3.97535147e-02 5.20318077e-02\n",
            " 7.82065663e-02 1.00343044e-01 4.92592570e-02 0.00000000e+00\n",
            " 3.04224615e-03 2.44156088e-04 2.20659896e-04 3.06472077e-06\n",
            " 1.02157359e-06 2.87062179e-04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QOEwDzIunyMX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "bbd513ca-8387-4dc8-cc8f-624ee84c6a83"
      },
      "cell_type": "code",
      "source": [
        "_, seq_len, num_letters = x_train.shape\n",
        "l = 2\n",
        "output_dim = ys[l].shape[-1]\n",
        "\n",
        "inputs = Input(shape=(seq_len, num_letters), name='input')\n",
        "dropout = Dropout(0.3)(inputs)\n",
        "#attention\n",
        "dense = TimeDistributed(Dense(5, activation='tanh'), name='attention_input_dense')(inputs)\n",
        "reshape = Reshape(target_shape=(-1, ), name='attention_input_reshape')(dense)\n",
        "attention_probs = Dense(seq_len, activation='softmax')(reshape)\n",
        "attention_probs = RepeatVector(num_letters, name='repeat_attention')(attention_probs)\n",
        "permute = Permute((2,1), name='permute_attention_inputs')(inputs)\n",
        "merge = Multiply(name='merge')([permute, attention_probs])\n",
        "permute = Permute((2,1), name='permute_attention_output')(merge)\n",
        "\n",
        "rnn1 = Bidirectional(lstm(64, return_sequences=True), name='rnn1')(dropout)\n",
        "rnn1 = Dropout(0.3)(rnn1)\n",
        "\n",
        "#attention_rnn = attention_mechanism(rnn1)\n",
        "\n",
        "rnn2 = Bidirectional(lstm(64, return_sequences=True), name='rnn2')(rnn1)\n",
        "rnn2 = Dropout(0.3)(rnn2)\n",
        "\n",
        "outputs = Dense(output_dim, activation='softmax', name='output')(rnn2)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "print(model.summary())\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(x_train, ys_train[l], validation_data=(x_val, ys_val[l]), batch_size=32, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Az8n8kyGFsxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, ys_train[l], validation_data=(x_val, ys_val[l]), batch_size=32, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Lv5P0kbt2jr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save(\"attention_model_1.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7pN_OE1tG6a0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_outputs = 3\n",
        "input_ = Input(shape = x_train.shape[1:])\n",
        "x = input_\n",
        "x = Bidirectional(lstm(64, input_shape = (maxlen, input_dim), return_sequences=True, kernel_regularizer = l2(0.02), recurrent_regularizer = l2(0.02)))(x)\n",
        "x_s = [layer(32)(x) for i in range(n_outputs)]\n",
        "x_s = [layer(32)(x) for x in x_s]\n",
        "#x_s = [Dropout(0.5)(layer(32)(x)) for x in x_s]\n",
        "dense_layers = [Dense(output_dims[i])(x_s[i]) for i in range(n_outputs)]\n",
        "output_layers = [Activation('softmax', name=str(i))(dense) for i, dense in enumerate(dense_layers)]\n",
        "\n",
        "model = Model(inputs = [input_], outputs = output_layers)\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZdVg-xzt_El2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "דברים לנסות:\n",
        "activity regularization\n",
        "skip connections\n",
        "gaussian noise in the first layer instead of dropout \n",
        "maybe constant recurrent dropout  (variational LSTM)\n"
      ]
    },
    {
      "metadata": {
        "id": "SiCOQvpWrC50",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "k = 2\n",
        "model = Sequential()\n",
        "#model.add(GaussianNoise(0.000, input_shape=(maxlen, input_dim)))\n",
        "model.add(Bidirectional(lstm(48, return_sequences=True, activity_regularizer=l2(0.000)), input_shape=(maxlen, input_dim)))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(lstm(48, return_sequences = True, activity_regularizer=l2(0.000))))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(TimeDistributed(Dense(output_dims[k], kernel_regularizer=l2(0.000000))))\n",
        "model.add(Activation('softmax'))\n",
        "print(ys_val[k].shape)\n",
        "optimizer = Nadam(lr=0.01, clipnorm=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(x_train, ys_train[k], validation_data=(x_val, ys_val[k]), batch_size = 64, epochs = 10, callbacks=[TerminateOnNaN()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01_BSVCS_AEG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "models, histories = [model],[history]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DIhB0NLJ60PX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wSfXR-xGGe0E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Y0E4iTOPHr-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model.get_weights()[3][0,:])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ice0PFNEOAwq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history2 = model.fit(x_train, ys_train[k], validation_data=(x_val, ys_val[k]), batch_size = 64, epochs = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7-QY2ok3vVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history1 = model.fit(x_train, ys_train[k], validation_data=(x_val, ys_val[k]), batch_size = 64, epochs = 10, callbacks=[TerminateOnNaN()])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "caYVuJjOsPWi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "history = model.fit([x_train], ys_train, validation_data=([x_val], ys_val), batch_size = 1024, epochs = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZsAiAtdhLF3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.evaluate([x_test], ys_test[k])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k6X0l1QFmd3-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Yu7X_MHTmhcz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model.layers)\n",
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('model train vs validation loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'validation'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qG6BAGj5g-Xg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ys_test_predict = [model.predict(x_test, batch_size=64) for model in models]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kXIhqHwJDPkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pointings_unicode = [[32, 1468], \n",
        "             [32, 1463, 1464, 1468, 1473, 1474], \n",
        "             [32, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1473, 1474]]\n",
        "letters_unicode = [32] + list(range(1488, 1515))\n",
        "def deTensorize(X, ys=[], k = 0):\n",
        "  if len(X.shape) == 3:\n",
        "    X = X[k, :, :]\n",
        "    ys = [y[k,: ,:] for y in ys]\n",
        "  N = X.shape[0]\n",
        "  num_letters = len(letters_unicode)\n",
        "  num_pointings = [len(pointing_unicode) for pointing_unicode in pointings_unicode]\n",
        "  index_letters = { i:chr(letters_unicode[i]) for i in range(num_letters)}\n",
        "  index_pointings = [{ i:chr(pointing_unicode[i]) for i in range(num_pointings[j])} for j, pointing_unicode in enumerate(pointings_unicode)]\n",
        "  letters = [index_letters[id] for id in np.argmax(X, axis=1)]\n",
        "  pointings = [[index_pointing[id] for id in np.argmax(ys[i], axis=1)] for i, index_pointing in enumerate(index_pointings)]\n",
        "  pointing_seqs = [''.join([pointings[j][i] for j in range(3)]) for i in range(N)]\n",
        "  pointing_seqs = [seq.lstrip() for seq in pointing_seqs]\n",
        "  text = ''.join([''.join(l) for l in zip(letters, pointing_seqs)])\n",
        "  print(text)\n",
        "  print(''.join(letters))\n",
        "k = 987\n",
        "deTensorize(x_test, ys_test, k)\n",
        "deTensorize(x_test, ys_test_predict, k)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4c1rbzprg9g6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
        "                        input_shape=(5, 10)))\n",
        "model.add(Bidirectional(LSTM(10)))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PeGtIdHkAk5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}