{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysterin/deep-pointing/blob/master/deep_pointing_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KaJQH-LDKeHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "76947415-a9cf-468e-9466-7fcb2d210604"
      },
      "cell_type": "code",
      "source": [
        "!pip install line-profiler\n",
        "%load_ext line_profiler\n",
        "import numpy as np\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn import Module\n",
        "from torch.autograd import Variable as V\n",
        "accelerator\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting line-profiler\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/fc/ecf4e238bb601ff829068e5a72cd1bd67b0ee0ae379db172eb6a0779c6b6/line_profiler-2.1.2.tar.gz (83kB)\n",
            "\r\u001b[K    12% |████                            | 10kB 20.1MB/s eta 0:00:01\r\u001b[K    24% |███████▉                        | 20kB 4.2MB/s eta 0:00:01\r\u001b[K    36% |███████████▉                    | 30kB 5.9MB/s eta 0:00:01\r\u001b[K    49% |███████████████▊                | 40kB 3.9MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▊            | 51kB 4.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 61kB 5.6MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▋    | 71kB 6.3MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 81kB 7.0MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 92kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython>=0.13 in /usr/local/lib/python3.6/dist-packages (from line-profiler) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (1.0.15)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (40.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (4.3.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (4.3.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython>=0.13->line-profiler) (4.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13->line-profiler) (0.1.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython>=0.13->line-profiler) (1.11.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython>=0.13->line-profiler) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython>=0.13->line-profiler) (0.6.0)\n",
            "Building wheels for collected packages: line-profiler\n",
            "  Running setup.py bdist_wheel for line-profiler ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/05/7d/9b/aafbe8d78dc2b2c644d2efd2f060ab3258143860142575193a\n",
            "Successfully built line-profiler\n",
            "Installing collected packages: line-profiler\n",
            "Successfully installed line-profiler-2.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cu80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "9letQM4y8QBq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1t4zss4j8GkqkPEKIBXpYD6brm2G3Y8DG'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentFile('wikisource.zip')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJIdTkdQ837L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip wikisource.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mwYOirN1m3Xk",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7edda418-9cfe-41cb-ed23-a44582b406d1"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-61e50189-3d47-4de9-93fb-0d2a83637c7f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-61e50189-3d47-4de9-93fb-0d2a83637c7f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ta1.txt to ta1.txt\n",
            "User uploaded file \"ta1.txt\" with length 3062255 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pMKrqJzT8S1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b666486-7453-4742-a44e-aef9ae8bc762"
      },
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "import os\n",
        "from os import path\n",
        "text = ''\n",
        "files_list = os.listdir('./wikisource')\n",
        "shuffle(files_list)\n",
        "texts = []\n",
        "for f in files_list:\n",
        "  with open(\"./wikisource/\"+f, 'rb') as file:\n",
        "    t = file.read().decode('utf-8')\n",
        "    texts.append(t)\n",
        "    if len(t) > 10000:\n",
        "      text += t\n",
        "len(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46464140"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "VYxXL1ca8bjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73fe1b86-2e41-4083-9b23-8aea83f7152f"
      },
      "cell_type": "code",
      "source": [
        "text = open(\"ta1.txt\", 'r').read()\n",
        "len(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1630922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "7BHygtZC8h0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9f0b662f-6028-48c6-9a1c-143db71ae118"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = re.sub(chr(64288), chr(1506), text)\n",
        "text = re.sub(chr(64291), chr(1492), text)\n",
        "chars = list(set(text))\n",
        "letters, pointings = [chr(i) for i in range(ord('א'), ord('ת') + 1)], [chr(i) for i in list(range(1455, 1468)) + [1479, 1456]]\n",
        "print( letters, pointings)\n",
        "text = ''.join([c for c in text if c in letters or c in pointings or c in [' ', '\\n', '\\t']])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'ך', 'כ', 'ל', 'ם', 'מ', 'ן', 'נ', 'ס', 'ע', 'ף', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ש', 'ת'] ['֯', 'ְ', 'ֱ', 'ֲ', 'ֳ', 'ִ', 'ֵ', 'ֶ', 'ַ', 'ָ', 'ֹ', 'ֺ', 'ֻ', 'ׇ', 'ְ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mg_AIi9b8olh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b0cb9659-6496-4937-b11d-684d4e9523c4"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "chars = sorted(set(text))\n",
        "pointing_chars = [c for c in chars if 1455<ord(c)<1468 or ord(c) in [1479, 65533]]\n",
        "non_pointing_chars = [c for c in chars if not (1455<ord(c)<1480 or ord(c) == 65533) ]\n",
        "pointing_chars, non_pointing_chars\n",
        "special_chars = ['.', '[',']','}', '{', '(', ')', '\\\\', '\\/']\n",
        "np_chars_pattern = ['\\\\'+c for c in non_pointing_chars if c in special_chars] + [c for c in non_pointing_chars if c not in special_chars]\n",
        "print(np_chars_pattern)\n",
        "np_pattern = '[' + ''.join(np_chars_pattern) + ']'\n",
        "np_pattern\n",
        "\n",
        "unpointed_text = ''.join(re.findall(np_pattern, text))\n",
        "pointings = re.split(np_pattern, text)[1:]\n",
        "print(len(unpointed_text), len(pointings))\n",
        "len(''.join([''.join(z) for z in zip(pointings, unpointed_text)])) == len(text)\n",
        "_text = ''.join([''.join(z) for z in zip(unpointed_text, pointings)])\n",
        "for i in range(len(text)):\n",
        "  if _text[i] != text[i]:\n",
        "    print(i, text[i], _text[i])\n",
        "    \n",
        "ps2ids = {p:i for i, p in enumerate(pointing_chars)}\n",
        "cs2ids = {c:i for i, c in enumerate(non_pointing_chars)}\n",
        "ids2ps = {i:p for i, p in enumerate(pointing_chars)}\n",
        "ids2cs = {i:c for i, c in enumerate(non_pointing_chars)}\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras import utils\n",
        "vocab_size = len(cs2ids)\n",
        "X = np.asarray([cs2ids[c] for c in unpointed_text], dtype=np.int64)\n",
        "#X = utils.to_categorical(X)\n",
        "N_points = len(pointing_chars)\n",
        "N = len(unpointed_text)\n",
        "y = np.zeros(N, dtype=np.int64)\n",
        "for i in range(N):\n",
        "  for j in range(N_points):\n",
        "    if ids2ps[j] in pointings[i]:\n",
        "      y[i] = j+1\n",
        "      break\n",
        "      \n",
        "print(y[:10])\n",
        "print(X[:10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' ', 'א', 'ב', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'ך', 'כ', 'ל', 'ם', 'מ', 'ן', 'נ', 'ס', 'ע', 'ף', 'פ', 'ץ', 'צ', 'ק', 'ר', 'ש', 'ת']\n",
            "978882 978882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0 1 6 0 5 0 0 0 9 9]\n",
            "[ 0  2 25  1 26 10 27  0  2 25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bcp6tnLoAAHi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "c = Counter(text)\n",
        "print (sum([n for p, n in c.items() if p in non_pointing_chars]))\n",
        "print (sum([n for p, n in c.items()]))\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VE7o2SL88ryI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34f87907-b6c9-45a1-f67e-3b0f8490cc15"
      },
      "cell_type": "code",
      "source": [
        "from numpy.random import permutation\n",
        "from keras.utils import Sequence, to_categorical\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return np.eye(num_classes, dtype='float32')[y]\n",
        "def max_(arr):\n",
        "  if isinstance(arr, torch._TensorBase):\n",
        "    return torch.max(arr)\n",
        "  return np.max(arr)\n",
        "def concat(*arrs):\n",
        "  if isinstance(arrs[0], (torch.autograd.Variable, torch._TensorBase)):\n",
        "    return torch.stack(arrs)\n",
        "  return np.asarray(arrs)\n",
        "def concat1(arrs):\n",
        "  if isinstance(arrs[0], (torch.autograd.Variable, torch._TensorBase)):\n",
        "    return torch.stack(arrs)\n",
        "  return np.asarray(arrs)\n",
        "print(max_(torch.from_numpy(np.array([[1,2,3],[4,5,6]]))))\n",
        "class batchSeq(Sequence):\n",
        "    def __init__(self, X, y, length, stride, batch_size, randlen=True, shuffle = True, torch=False, x_categorical=False, y_categorical=False):\n",
        "      self._N = X.shape[0]\n",
        "      self.X = X\n",
        "      self.y = y\n",
        "      self.length = length\n",
        "      self.stride = stride\n",
        "      self.bs = batch_size\n",
        "      self.index = 0\n",
        "      self.randlen = randlen\n",
        "      self.shuffle = shuffle\n",
        "      self.torch = torch\n",
        "      self.x_categorical = x_categorical\n",
        "      self.y_categorical = y_categorical\n",
        "      self.vocab_size = max_(X) + 1\n",
        "      self.y_dim = max_(y) + 1\n",
        "    \n",
        "    def __len__(self):\n",
        "      return ((self._N-self.length)//self.stride)//self.bs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      length = self.length\n",
        "\n",
        "      if self.randlen:\n",
        "        length = np.random.randint(length-10, length+10)\n",
        "        if np.random.rand(1)[0] < 0.05:\n",
        "          length = length//2\n",
        "      bx = concat1([self.X[self.stride*(self.bs*idx+i):self.stride*(self.bs*idx+i)+length] for i in range(self.bs)])\n",
        "      by = concat1([self.y[self.stride*(self.bs*idx+i):self.stride*(self.bs*idx+i)+length] for i in range(self.bs)])\n",
        "      if self.x_categorical:\n",
        "        bx = to_categorical(bx, self.vocab_size)\n",
        "      if self.y_categorical:\n",
        "        by = to_categorical(by, self.y_dim)\n",
        "      if self.torch:\n",
        "        bx = V(torch.from_numpy(bx).cuda())\n",
        "        by = V(torch.from_numpy(by).cuda())\n",
        "      return bx, by\n",
        "    \n",
        "    def __iter__(self):\n",
        "      self.index = 0\n",
        "      if self.shuffle: self.permute = permutation(self.__len__())\n",
        "      return self\n",
        "    \n",
        "    def __next__(self):\n",
        "      if self.index < self.__len__():\n",
        "        self.index += 1\n",
        "        if self.shuffle: return self.__getitem__(self.permute[self.index-1])\n",
        "        else: return self[self.index-1]\n",
        "      else:\n",
        "        raise StopIteration"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KCyX1cRR9CJt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = X.astype(np.int64)\n",
        "y = y.astype(np.int64)\n",
        "X = torch.from_numpy(X).cuda()\n",
        "y = torch.from_numpy(y).cuda()\n",
        "val_id = int(X.shape[0]*0.1)\n",
        "test_id = int(X.shape[0]*0.8)\n",
        "X_val, X_trn, X_test = X[:val_id], X[val_id:test_id], X[test_id:]\n",
        "y_val, y_trn, y_test = y[:val_id], y[val_id:test_id], y[test_id:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bsz2E7XO9Kzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74ba8e3a-08d6-4615-c98f-ddd7da22bb71"
      },
      "cell_type": "code",
      "source": [
        "val_id, test_id"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2696966, 21575735)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "134afSFis7_S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyRNN(Module):\n",
        "  def __init__(self, inpsize, hidsize, activation='tanh'):\n",
        "    super().__init__()\n",
        "    self.hidsize = hidsize\n",
        "    self.w_ih = nn.Parameter(torch.zeros((hidsize, inpsize)).cuda())\n",
        "    torch.nn.init.xavier_normal(self.w_ih)\n",
        "    self.w_hh = nn.Parameter(torch.eye(hidsize).cuda())\n",
        "    self.bias = nn.Parameter(torch.zeros(hidsize).cuda())\n",
        "    if activation=='tanh':\n",
        "      self.s = F.tanh\n",
        "    elif activation=='relu':\n",
        "      self.f = F.relu\n",
        "  def forward(self, x, h=None):\n",
        "    if h is None:\n",
        "      h = V(torch.zeros(self.hidsize).cuda(), requires_grad=False)\n",
        "    return self.s(F.linear(x, self.w_ih, self.bias) + F.linear(h, self.w_hh))\n",
        "# my = MyRNN(8, 12)\n",
        "# list(my.parameters())\n",
        "# x = V(torch.randn((16, inps)).cuda())\n",
        "# h0 = V(torch.randn((16, hids)).cuda())\n",
        "# my(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NzyUPYOMMppj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "class RecurrentLayer(Module):\n",
        "  def __init__(self, cell, return_sequence=True, dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.ret_seq = return_sequence\n",
        "    self.cell = cell\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x, h0=None):\n",
        "    bs, bptt, inpdim = x.size()\n",
        "    hidden = h0\n",
        "    hiddens = []\n",
        "    for i in range(bptt):\n",
        "      hidden = self.cell(x[:,i,:], hidden)  \n",
        "      hiddens.append(hidden)\n",
        "    if isinstance(hidden, tuple):\n",
        "      ys = [h[0] for h in hiddens]\n",
        "      y = hidden[0]\n",
        "    else:\n",
        "      ys = hiddens\n",
        "      y = hidden\n",
        "    if self.ret_seq: return torch.stack(ys, 1)\n",
        "    else:            return y\n",
        "\n",
        "# inps = 8\n",
        "# hids = 12\n",
        "# l = RecurrentLayer(MyRNN(inps, hids))\n",
        "# x = V(torch.randn((16, 20, inps)).cuda())\n",
        "# h0 = V(torch.randn((16, hids)).cuda())\n",
        "# l(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnqJnCzFRIjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pdb\n",
        "from torch import nn\n",
        "from torch.nn import Module\n",
        "from torch.distributions import Bernoulli\n",
        "from scipy.stats import bernoulli\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable as V\n",
        "def dropout(tensor, mask, p, training):\n",
        "  if training:\n",
        "    return tensor*mask\n",
        "  else:\n",
        "    return tensor*(1-p)\n",
        "def sample_bernoulli(dropout, shape):\n",
        "  p = 1 - dropout\n",
        "  data = bernoulli.rvs(p, size=shape).astype(np.float32)/p\n",
        "  return V(torch.from_numpy(data).cuda(), requires_grad=False)\n",
        "  \n",
        "class BidirectionalLSTMlayer(Module):\n",
        "  def __init__(self, inpsize, hidsize, in_dropout=0.0, rec_dropout=0.0, out_dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.inpsize = inpsize\n",
        "    self.hidsize = hidsize\n",
        "    self.cell_fw = nn.LSTMCell(inpsize, hidsize)\n",
        "    self.cell_bw = nn.LSTMCell(inpsize, hidsize)\n",
        "    self.in_dpout, self.rec_dpout, self.out_dpout = in_dropout, rec_dropout, out_dropout\n",
        "#     self.in_dpout_dist = Bernoulli(torch.ones(self.inpsize)*(1-self.in_dpout))\n",
        "#     self.rec_dpout_dist = Bernoulli(torch.ones(self.hidsize)*(1-self.rec_dpout))\n",
        "#     self.out_dpout_dist = Bernoulli(torch.ones(self.hidsize)*(1-self.out_dpout))\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "    bs, bptt, indim = x.size()\n",
        "    hfw = V(torch.zeros((bs, self.hidsize)).cuda(), requires_grad=True)\n",
        "    cfw = V(torch.zeros((bs, self.hidsize)).cuda(), requires_grad=True)\n",
        "    hbw = V(torch.zeros((bs, self.hidsize)).cuda(), requires_grad=True)\n",
        "    cbw = V(torch.zeros((bs, self.hidsize)).cuda(), requires_grad=True)    \n",
        "    hmaskf = sample_bernoulli(self.rec_dpout, (bs, self.hidsize))\n",
        "    xmaskf = sample_bernoulli(self.in_dpout, (bs, self.inpsize))\n",
        "    outmaskf = sample_bernoulli(self.out_dpout, (bs, self.hidsize))\n",
        "    hmaskb = sample_bernoulli(self.rec_dpout, (bs, self.hidsize))\n",
        "    xmaskb = sample_bernoulli(self.in_dpout, (bs, self.inpsize))\n",
        "    outmaskb = sample_bernoulli(self.out_dpout, (bs, self.hidsize))\n",
        "    hs_fwd, hs_bwd = [], []\n",
        "    \n",
        "    for i in range(bptt):\n",
        "      xtf = x[:,i,:]\n",
        "      xtb = x[:,-i-1,:]\n",
        "      if self.training:\n",
        "        hfw = hfw*hmaskf\n",
        "        hbw = hbw*hmaskb\n",
        "        xtf = xtf*xmaskf\n",
        "        xtb = xtb*xmaskb\n",
        "      hfw, cfw = self.cell_fw(xtf, (hfw, cfw))\n",
        "      if self.training: hs_fwd.append(hfw*outmaskf)\n",
        "      else:             hs_fwd.append(hfw)\n",
        "      hbw, cbw = self.cell_bw(xtb, (hbw, cbw))\n",
        "      if self.training: hs_bwd.append(hbw*outmaskb)\n",
        "      else:             hs_bwd.append(hbw)\n",
        "    ys = [torch.cat((hfw, hbw), -1) for hfw, hbw in zip(hs_fwd, hs_bwd)]\n",
        "    return torch.stack(ys, 1)\n",
        "inps = 8\n",
        "hids = 12\n",
        "hfw = V(torch.zeros((16, hids)).cuda(), requires_grad=False)\n",
        "cfw = V(torch.zeros((16, hids)).cuda(), requires_grad=False)\n",
        "x = V(torch.randn((16, 20, inps)).cuda())\n",
        "l = BidirectionalLSTMlayer(inps, hids, in_dropout=0.3).cuda()\n",
        "#n = nn.GRUCell(inps, hids).cuda()\n",
        "#pdb.set_trace()\n",
        "# l(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9reJ12DN5q3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3c2ac3a7-5da7-425f-dd83-0136e7e10610"
      },
      "cell_type": "code",
      "source": [
        "sample_bernoulli(0.5, (4, 5))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Variable containing:\n",
              " 0  2  2  0  2\n",
              " 2  2  2  2  0\n",
              " 2  0  0  0  2\n",
              " 0  0  0  2  2\n",
              "[torch.cuda.FloatTensor of size 4x5 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "BWmlAx1GtBrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BidirectionalGRUlayer(Module):\n",
        "  def __init__(self, inpsize, hidsize, in_dropout=0.0, rec_dropout=0.0, out_propout=0.0):\n",
        "    super().__init__()\n",
        "    self.inpsize = inpsize\n",
        "    self.hidsize = hidsize\n",
        "    self.cell_fw = nn.GRUCell(inpsize, hidsize)\n",
        "    self.cell_bw = nn.GRUCell(inpsize, hidsize)\n",
        "    self.in_dpout, self.rec_dpout, self.out_dpout = in_dropout, rec_dropout, out_dropout\n",
        "    \n",
        "  def forward(self, x):\n",
        "    bs, bptt, _ = x.size()\n",
        "    hfw = V(torch.zeros((bs, self.hidsize)).cuda(), requires_grad=True)\n",
        "    hbw = V(torch.zeros((bs, self.hidsize)).cuda(), requires_grad=True)\n",
        "    hs_fwd, hs_bwd = [], []\n",
        "    \n",
        "    for i in range(bptt):\n",
        "      hfw = self.cell_fw(x[:,i,:], hfw)\n",
        "      hs_fwd.append(hfw)\n",
        "      hbw, cbw = self.cell_bw(x[:,-i-1,:], hbw)\n",
        "      hs_bwd.insert(0, hbw)\n",
        "    ys = [torch.cat((hfw, hbw), -1) for hfw, hbw in zip(hs_fwd, hs_bwd)]\n",
        "    return torch.stack(ys, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ijSgaYjMVq5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wDGRQTJUf21I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BidirectionalLayer(Module):\n",
        "  def __init__(self, cell_fw, cell_bw, merge='concat', dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.cell_fw, self.cell_bw = cell_fw, cell_bw\n",
        "    self.merge = merge\n",
        "    self.dropout = dropout\n",
        "    \n",
        "  def forward(self, x, hfw0=None, hbw0=None):\n",
        "    bs, bptt, inpdim = x.size()\n",
        "    hid_fw, hid_bw = hfw0, hbw0\n",
        "    hids_fw, hids_bw = [], []\n",
        "    for i in range(bptt):\n",
        "      hid_fw = self.cell_fw(x[:,i,:], hid_fw)\n",
        "      if isinstance(hid_fw, tuple): hids_fw.append(hid_fw[0])\n",
        "      else:                         hids_fw.append(hid_fw)\n",
        "      hid_bw = self.cell_bw(x[:,-i-1,:], hid_bw)\n",
        "      if isinstance(hid_fw, tuple): hids_bw.append(hid_bw[0])\n",
        "      else:                         hids_bw.append(hid_bw)\n",
        "    if self.merge=='concat':\n",
        "      ys = [torch.cat((hfw, hbw), -1) for hfw, hbw in zip(hids_fw, hids_bw)]\n",
        "    return torch.stack(ys, 1)\n",
        "  \n",
        "# inps = 8\n",
        "# hids = 12\n",
        "# l = BidirectionalLayer(MyRNN(inps, hids), MyRNN(inps, hids))\n",
        "# x = V(torch.randn((16, 20, inps)).cuda())\n",
        "# h0 = V(torch.randn((16, hids)).cuda())\n",
        "# c0 = V(torch.randn((16, hids)).cuda())\n",
        "# l(x)\n",
        "# #[n for n, p in l.named_parameters()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A79WMfBgp0mt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "79b05a80-00b5-44c1-c781-bd993bf3147c"
      },
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/content/mtrand.pyx\u001b[0m(993)\u001b[0;36mmtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
            "\n",
            "ipdb> exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ipa1056UQFs8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def myBdLSTM(inpsize, outsize):\n",
        "  cell_fw = nn.LSTMCell(inpsize, outsize)\n",
        "  cell_bw = nn.LSTMCell(inpsize, outsize)\n",
        "  return BidirectionalLayer(cell_fw, cell_bw)\n",
        "def myBdRNN(inpsize, outsize):\n",
        "  cell_fw = MyRNN(inpsize, outsize)\n",
        "  cell_bw = MyRNN(inpsize, outsize)\n",
        "  return BidirectionalLayer(cell_fw, cell_bw)\n",
        "def myBDGRU(inpsize, outsize):\n",
        "  cell_fw = nn.GRUCell(inpsize, outsize)\n",
        "  cell_bw = nn.GRUCell(inpsize, outsize)\n",
        "  return BidirectionalLayer(cell_fw, cell_bw)\n",
        "class charModelBi(Module):\n",
        "  def __init__(self, vocab_size, nfac, nhidden, num_layers, output_dim, res = True, dropout = False):\n",
        "    super().__init__()\n",
        "    self.nhidden = nhidden\n",
        "    self.num_layers = num_layers\n",
        "    self.emb = nn.Embedding(vocab_size, nfac)\n",
        "    inp_sizes = [nfac] + [nhidden*2]*(num_layers-1)\n",
        "    hid_sizes = [nhidden]*num_layers\n",
        "    if dropout:\n",
        "      self.rnns = nn.ModuleList([BidirectionalLSTMlayer(insz, hidsz, rec_dropout=0.4, in_dropout=0.04, out_dropout=0.4) for insz, hidsz in zip(inp_sizes, hid_sizes)])\n",
        "    else:\n",
        "      self.rnns = nn.ModuleList([nn.LSTM(insz, hidsz, 1, batch_first=True, bidirectional=True) for insz, hidsz in zip(inp_sizes, hid_sizes)])\n",
        "    #self.norms = nn.ModuleList([nn.InstanceNorm1d(hidsz) for hidsz in hid_sizes])\n",
        "    self.out = nn.Linear(nhidden*2, output_dim)\n",
        "    self.res = res\n",
        "  \n",
        "  def forward(self, x):\n",
        "    bs = x.size(0)\n",
        "    bptt = x.size(1)\n",
        "    h0 = F.Variable(torch.zeros((bs, self.nhidden)).cuda())\n",
        "    c0 = F.Variable(torch.zeros((bs, self.nhidden)).cuda())\n",
        "    x = self.emb(x)\n",
        "    xs = []\n",
        "    for i, rnn in enumerate(self.rnns):\n",
        "      out = rnn(x)\n",
        "      if isinstance(out, tuple):\n",
        "        out = out[0]\n",
        "      if i==0 or not self.res: x = out\n",
        "      else:                    x = x + out\n",
        "\n",
        "    return F.log_softmax(self.out(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ZTLbLGFzOgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, seq, n=0):\n",
        "  model.eval()\n",
        "  acc_loss, acc_acc = 0, 0\n",
        "  i = 0\n",
        "  for bx, by in seq:\n",
        "    i += 1\n",
        "    \n",
        "    #pdb.set_trace()\n",
        "    outputs = model(V(bx))\n",
        "    target = V(by)\n",
        "    outputs, target = outputs[:, n:-n-1].contiguous(), target[:, n:-n-1].contiguous()\n",
        "    loss = nll_loss_seq(outputs, target)\n",
        "    acc_loss += loss.data.cpu().numpy()[0]\n",
        "    acc = accuracy(outputs, target)\n",
        "    acc_acc += acc.data.cpu().numpy()[0]\n",
        "  print(\"avarage loss: {}\".format(acc_loss/i))\n",
        "  print(\"avarage accuracy: {}\".format(acc_acc/i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsT_hB49-GXu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nll_loss_seq(inp, targ):\n",
        "  #print(inp.size(), targ.size())\n",
        "  sl,bs,nh = inp.size()\n",
        "  #targ = targ.transpose(0,1).contiguous().view(-1)\n",
        "  targ = targ.view(-1)\n",
        "  return F.nll_loss(inp.view(-1,nh), targ)\n",
        "\n",
        "def cross_entropy_seq(inp, targ):\n",
        "  #print(inp.size(), targ.size())\n",
        "  sl,bs,nh = inp.size()\n",
        "  targ = targ.transpose(0,1).contiguous().view(-1)\n",
        "  return nn.CrossEntropyLoss()(inp.view(-1,nh), targ)\n",
        "\n",
        "def accuracy(inp, targ):\n",
        "  _, inp = torch.max(inp, dim=-1)\n",
        "  return torch.mean(torch.eq(inp, targ).double())\n",
        "\n",
        "def fit(model, lr, trn_seq, val_seq=None, epochs=1, batches_per_epoch=-1, wd=0.0):\n",
        "  if batches_per_epoch < 1:\n",
        "    batches_per_epoch = len(trn_seq)\n",
        "  print(\"number of batches: \", len(trn_seq)*epochs)\n",
        "  model.cuda()\n",
        "  interval = 200\n",
        "  opt = optim.Adam(model.parameters(), lr=lr)\n",
        "  #scheduler = optim.lr_scheduler.StepLR(opt, interval*3, gamma=0.8)\n",
        "  losses = []\n",
        "  i = 0\n",
        "  for ie in range(epochs):\n",
        "    i = 0\n",
        "    acc_i = 0\n",
        "    acc_loss, acc_acc, acc_i = 0, 0, 0\n",
        "    for bx, by in trn_seq:\n",
        "      i += 1\n",
        "      #scheduler.step()\n",
        "      if i%batches_per_epoch==0:\n",
        "        if batches_per_epoch < interval:\n",
        "          print(\"avarage training loss at epoch     {}: {}\".format(ie, acc_loss/acc_i))\n",
        "          print(\"avarage training accuracy at epoch {}: {}\".format(ie, acc_acc/acc_i))\n",
        "        if val_seq:\n",
        "          evaluate(model, val_seq)\n",
        "      model.train()\n",
        "      inp = V(bx)\n",
        "      opt.zero_grad()\n",
        "      outputs = model(inp)\n",
        "      target = V(by)\n",
        "      loss = nll_loss_seq(outputs, target)\n",
        "      acc = accuracy(outputs, target)\n",
        "      losses.append(loss.data.cpu().numpy()[0])\n",
        "      acc_acc += acc.data.cpu().numpy()[0]\n",
        "      acc_loss += loss.data.cpu().numpy()[0]\n",
        "      acc_i += 1\n",
        "      opt.zero_grad()\n",
        "      loss.backward()\n",
        "      for name, param in model.named_parameters():\n",
        "        if name.split('.')[-1].startswith('weight'): \n",
        "          param.data.add_(-lr*wd*param.data)\n",
        "      opt.step()\n",
        "      if i%interval==0:\n",
        "        print(\"avarage loss at i = {}: {}\".format(i, acc_loss/interval))\n",
        "        print(\"avarage accuracy at i = {}: {}\".format(i, acc_acc/interval))\n",
        "        acc_loss, acc_acc = 0, 0\n",
        "\n",
        "      \n",
        "  return losses, [p.data.cpu().numpy() for p in model.parameters()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjVcxwAR-O9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        },
        "outputId": "f0f5dd4d-5633-4a83-97f0-950b41dc003f"
      },
      "cell_type": "code",
      "source": [
        "def num_params(model):\n",
        "  return sum([np.prod(p.size()) for p in model.parameters()])\n",
        "#num_params(charModelBi(vocab_size, 512, 256, 8, N_points + 1))\n",
        "vocab_size = len(cs2ids)\n",
        "bptt = 60\n",
        "output_dim = N_points + 1\n",
        "model = charModelBi(vocab_size, 128, 1024, 3, output_dim, res=False, dropout=True).cuda()\n",
        "print(num_params(model))\n",
        "i = np.random.randint(len(X_trn)-100000)\n",
        "trn_seq = batchSeq(X_trn, y_trn, bptt, bptt, 64, randlen=True)\n",
        "print(len(trn_seq))\n",
        "val_seq = batchSeq(X_val, y_val, bptt, bptt, 128, randlen=False)\n",
        "#model(V(trn_seq[0][0]))\n",
        "#torch.save(model.state_dict(), \"my model.pt\")\n",
        "%lprun -f fit losses, _ =  fit(model, 5e-4, trn_seq, val_seq=val_seq, epochs=20, wd=0, batches_per_epoch=len(trn_seq))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59848205\n",
            "178\n",
            "number of batches:  3560\n",
            "avarage training loss at epoch     0: 1.072557281302867\n",
            "avarage training accuracy at epoch 0: 0.6516658744071547\n",
            "avarage loss: 0.6232630362113317\n",
            "avarage accuracy: 0.7847921080508474\n",
            "avarage training loss at epoch     1: 0.5472142113804144\n",
            "avarage training accuracy at epoch 1: 0.8145216334444574\n",
            "avarage loss: 0.46787432581186295\n",
            "avarage accuracy: 0.8439044844632769\n",
            "avarage training loss at epoch     2: 0.432099685830585\n",
            "avarage training accuracy at epoch 2: 0.8558117662312184\n",
            "avarage loss: 0.39659195641676587\n",
            "avarage accuracy: 0.8664371468926553\n",
            "avarage training loss at epoch     3: 0.3655796631918115\n",
            "avarage training accuracy at epoch 3: 0.8779202462110822\n",
            "avarage loss: 0.35909072309732437\n",
            "avarage accuracy: 0.881642831920904\n",
            "avarage training loss at epoch     4: 0.3240314438349783\n",
            "avarage training accuracy at epoch 4: 0.8917984767711281\n",
            "avarage loss: 0.3304392124215762\n",
            "avarage accuracy: 0.8902939618644067\n",
            "avarage training loss at epoch     5: 0.29300326614056604\n",
            "avarage training accuracy at epoch 5: 0.9016065606333981\n",
            "avarage loss: 0.3134106198946635\n",
            "avarage accuracy: 0.8963850635593219\n",
            "avarage training loss at epoch     6: 0.26685845952923015\n",
            "avarage training accuracy at epoch 6: 0.9104867990134813\n",
            "avarage loss: 0.2962077284852664\n",
            "avarage accuracy: 0.9006995939265536\n",
            "avarage training loss at epoch     7: 0.24734478397557966\n",
            "avarage training accuracy at epoch 7: 0.9164066999873255\n",
            "avarage loss: 0.2891973778605461\n",
            "avarage accuracy: 0.9050030896892655\n",
            "avarage training loss at epoch     8: 0.2283897659199386\n",
            "avarage training accuracy at epoch 8: 0.9228618912838363\n",
            "avarage loss: 0.2855646287401517\n",
            "avarage accuracy: 0.9074858757062146\n",
            "avarage training loss at epoch     9: 0.21385992886656421\n",
            "avarage training accuracy at epoch 9: 0.9276567006058397\n",
            "avarage loss: 0.27686640123526257\n",
            "avarage accuracy: 0.9108734992937854\n",
            "avarage training loss at epoch     10: 0.20081481640621768\n",
            "avarage training accuracy at epoch 10: 0.9317139004669731\n",
            "avarage loss: 0.28113310039043427\n",
            "avarage accuracy: 0.9102334922316385\n",
            "avarage training loss at epoch     11: 0.18942313582378592\n",
            "avarage training accuracy at epoch 11: 0.9356312437583403\n",
            "avarage loss: 0.27331677700082463\n",
            "avarage accuracy: 0.9125617937853107\n",
            "avarage training loss at epoch     12: 0.17947192464844655\n",
            "avarage training accuracy at epoch 12: 0.938398519473568\n",
            "avarage loss: 0.2723826455573241\n",
            "avarage accuracy: 0.9144045727401132\n",
            "avarage training loss at epoch     13: 0.17027511350852623\n",
            "avarage training accuracy at epoch 13: 0.9414649350133597\n",
            "avarage loss: 0.2720917711655299\n",
            "avarage accuracy: 0.9139852577683617\n",
            "avarage training loss at epoch     14: 0.16342003992890233\n",
            "avarage training accuracy at epoch 14: 0.9440179293555249\n",
            "avarage loss: 0.2756102383136749\n",
            "avarage accuracy: 0.9166225282485877\n",
            "avarage training loss at epoch     15: 0.15580820612146357\n",
            "avarage training accuracy at epoch 15: 0.9460519107357973\n",
            "avarage loss: 0.269501776744922\n",
            "avarage accuracy: 0.9173508121468926\n",
            "avarage training loss at epoch     16: 0.14854190380728177\n",
            "avarage training accuracy at epoch 16: 0.9485324852083828\n",
            "avarage loss: 0.272875614464283\n",
            "avarage accuracy: 0.9193591101694913\n",
            "avarage training loss at epoch     17: 0.14067385884496453\n",
            "avarage training accuracy at epoch 17: 0.9514797729280412\n",
            "avarage loss: 0.2703807167708874\n",
            "avarage accuracy: 0.9194032485875706\n",
            "avarage training loss at epoch     18: 0.13753451280674692\n",
            "avarage training accuracy at epoch 18: 0.9522003573090344\n",
            "avarage loss: 0.27800631895661354\n",
            "avarage accuracy: 0.9182666843220338\n",
            "avarage training loss at epoch     19: 0.1314285823938537\n",
            "avarage training accuracy at epoch 19: 0.9543872036266896\n",
            "avarage loss: 0.278924111276865\n",
            "avarage accuracy: 0.919193591101695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "erDNO2XIRUS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "1b69cadc-febe-431a-93b3-6bf9da9d55cf"
      },
      "cell_type": "code",
      "source": [
        "#model = charModelBi(vocab_size, 24, 256, 2, output_dim, res=False).cuda()\n",
        "#torch.save(model.state_dict(), \"my model.pt\")\n",
        "#trn_seq = batchSeq(X_trn, y_trn, bptt, bptt, 32, randlen=True)\n",
        "print(len(trn_seq))\n",
        "#val_seq = batchSeq(X_val[:val_id//4], y_val[:val_id//4], bptt, bptt, 32, randlen=False)\n",
        "#model.load_state_dict(torch.load(\"my model.pt\"))\n",
        "%lprun -f fit losses, _ =  fit(model, 5e-5, trn_seq, val_seq=val_seq, epochs=10, wd=0, batches_per_epoch=len(trn_seq))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "178\n",
            "number of batches:  1780\n",
            "avarage training loss at epoch     0: 0.11546000928781126\n",
            "avarage training accuracy at epoch 0: 0.9599804683756508\n",
            "avarage loss: 0.27698013186454773\n",
            "avarage accuracy: 0.9212680967514123\n",
            "avarage training loss at epoch     1: 0.11264948340627433\n",
            "avarage training accuracy at epoch 1: 0.9608672931208203\n",
            "avarage loss: 0.27716362476348877\n",
            "avarage accuracy: 0.9212018891242938\n",
            "avarage training loss at epoch     2: 0.10956004689221328\n",
            "avarage training accuracy at epoch 2: 0.9618551188739634\n",
            "avarage loss: 0.2791987198094527\n",
            "avarage accuracy: 0.9216874117231638\n",
            "avarage training loss at epoch     3: 0.10888363431486706\n",
            "avarage training accuracy at epoch 3: 0.9620681015954096\n",
            "avarage loss: 0.2791239048043887\n",
            "avarage accuracy: 0.9217205155367233\n",
            "avarage training loss at epoch     4: 0.10782709174742133\n",
            "avarage training accuracy at epoch 4: 0.962372800012569\n",
            "avarage loss: 0.2800520285964012\n",
            "avarage accuracy: 0.922018449858757\n",
            "avarage training loss at epoch     5: 0.10425092233607998\n",
            "avarage training accuracy at epoch 5: 0.9635424394784805\n",
            "avarage loss: 0.2818318096299966\n",
            "avarage accuracy: 0.922018449858757\n",
            "avarage training loss at epoch     6: 0.1034726098727035\n",
            "avarage training accuracy at epoch 6: 0.9639092588313216\n",
            "avarage loss: 0.28204526503880817\n",
            "avarage accuracy: 0.9220515536723163\n",
            "avarage training loss at epoch     7: 0.10343026321402378\n",
            "avarage training accuracy at epoch 7: 0.9640050447091486\n",
            "avarage loss: 0.2829981992642085\n",
            "avarage accuracy: 0.9220515536723165\n",
            "avarage training loss at epoch     8: 0.10093139713935259\n",
            "avarage training accuracy at epoch 8: 0.9648336718122914\n",
            "avarage loss: 0.28520483523607254\n",
            "avarage accuracy: 0.9225701800847457\n",
            "avarage training loss at epoch     9: 0.10104143175449075\n",
            "avarage training accuracy at epoch 9: 0.9647444555904222\n",
            "avarage loss: 0.2847453976670901\n",
            "avarage accuracy: 0.922073622881356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jgziRbfssPCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        },
        "outputId": "66734808-b508-436e-ce7b-6be89e3b0ef4"
      },
      "cell_type": "code",
      "source": [
        "%lprun -f fit losses, _ =  fit(model, 1e-4, trn_seq, val_seq=val_seq, epochs=30, wd=0, batches_per_epoch=len(trn_seq))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of batches:  5340\n",
            "avarage training loss at epoch     0: 0.1016475196906739\n",
            "avarage training accuracy at epoch 0: 0.964546705920185\n",
            "avarage loss: 0.28640855848789215\n",
            "avarage accuracy: 0.9223163841807911\n",
            "avarage training loss at epoch     1: 0.09981147010447615\n",
            "avarage training accuracy at epoch 1: 0.9653146550165673\n",
            "avarage loss: 0.2886110097169876\n",
            "avarage accuracy: 0.922316384180791\n",
            "avarage training loss at epoch     2: 0.09614831575397718\n",
            "avarage training accuracy at epoch 2: 0.9665342119809365\n",
            "avarage loss: 0.2899316648642222\n",
            "avarage accuracy: 0.9223715572033897\n",
            "avarage training loss at epoch     3: 0.09511503162609655\n",
            "avarage training accuracy at epoch 3: 0.9666148571276607\n",
            "avarage loss: 0.2919873322049777\n",
            "avarage accuracy: 0.9231329449152543\n",
            "avarage training loss at epoch     4: 0.09409962838652444\n",
            "avarage training accuracy at epoch 4: 0.9670817219525788\n",
            "avarage loss: 0.292464683453242\n",
            "avarage accuracy: 0.9232322563559321\n",
            "avarage training loss at epoch     5: 0.09200993125553185\n",
            "avarage training accuracy at epoch 5: 0.9678020967176567\n",
            "avarage loss: 0.2935998427371184\n",
            "avarage accuracy: 0.9224046610169491\n",
            "avarage training loss at epoch     6: 0.09193952263748578\n",
            "avarage training accuracy at epoch 6: 0.9676771965563008\n",
            "avarage loss: 0.29632054393490154\n",
            "avarage accuracy: 0.922956391242938\n",
            "avarage training loss at epoch     7: 0.09005032761790659\n",
            "avarage training accuracy at epoch 7: 0.9683254200654944\n",
            "avarage loss: 0.29796286548177403\n",
            "avarage accuracy: 0.9231329449152542\n",
            "avarage training loss at epoch     8: 0.08869692133897442\n",
            "avarage training accuracy at epoch 8: 0.9691385199771114\n",
            "avarage loss: 0.3002124143143495\n",
            "avarage accuracy: 0.9232543255649716\n",
            "avarage training loss at epoch     9: 0.08944876534117144\n",
            "avarage training accuracy at epoch 9: 0.9687254452132679\n",
            "avarage loss: 0.30034905299544334\n",
            "avarage accuracy: 0.9228129413841808\n",
            "avarage training loss at epoch     10: 0.08812122754121231\n",
            "avarage training accuracy at epoch 10: 0.9691488596264175\n",
            "avarage loss: 0.30314934253692627\n",
            "avarage accuracy: 0.9226805261299437\n",
            "avarage training loss at epoch     11: 0.08562717416276366\n",
            "avarage training accuracy at epoch 11: 0.9698282345343106\n",
            "avarage loss: 0.30385660380125046\n",
            "avarage accuracy: 0.923188117937853\n",
            "avarage training loss at epoch     12: 0.08419279514227883\n",
            "avarage training accuracy at epoch 12: 0.9707083663352412\n",
            "avarage loss: 0.30458683396379155\n",
            "avarage accuracy: 0.9223936264124295\n",
            "avarage training loss at epoch     13: 0.08424400822338411\n",
            "avarage training accuracy at epoch 13: 0.9705292779913338\n",
            "avarage loss: 0.30562761053442955\n",
            "avarage accuracy: 0.9229122528248589\n",
            "avarage training loss at epoch     14: 0.08378456901274832\n",
            "avarage training accuracy at epoch 14: 0.9705518364943421\n",
            "avarage loss: 0.3095058500766754\n",
            "avarage accuracy: 0.9236957097457626\n",
            "avarage training loss at epoch     15: 0.08090641244319872\n",
            "avarage training accuracy at epoch 15: 0.9715316229396195\n",
            "avarage loss: 0.3070123828947544\n",
            "avarage accuracy: 0.9238170903954804\n",
            "avarage training loss at epoch     16: 0.0800075840604844\n",
            "avarage training accuracy at epoch 16: 0.9717322776828274\n",
            "avarage loss: 0.31094981729984283\n",
            "avarage accuracy: 0.9239053672316384\n",
            "avarage training loss at epoch     17: 0.07864055033289107\n",
            "avarage training accuracy at epoch 17: 0.972450204592244\n",
            "avarage loss: 0.31233516832192737\n",
            "avarage accuracy: 0.9230667372881357\n",
            "avarage training loss at epoch     18: 0.07948992645504785\n",
            "avarage training accuracy at epoch 18: 0.9720071933753194\n",
            "avarage loss: 0.31332561870416004\n",
            "avarage accuracy: 0.9235191560734463\n",
            "avarage training loss at epoch     19: 0.0781006464092745\n",
            "avarage training accuracy at epoch 19: 0.9725930353162179\n",
            "avarage loss: 0.31448621302843094\n",
            "avarage accuracy: 0.9234970868644069\n",
            "avarage training loss at epoch     20: 0.07576741431813455\n",
            "avarage training accuracy at epoch 20: 0.9735343327802594\n",
            "avarage loss: 0.3150686000784238\n",
            "avarage accuracy: 0.9235522598870056\n",
            "avarage training loss at epoch     21: 0.07611024116644752\n",
            "avarage training accuracy at epoch 21: 0.9731492804018457\n",
            "avarage loss: 0.31913429995377857\n",
            "avarage accuracy: 0.9233757062146895\n",
            "avarage training loss at epoch     22: 0.0761067033192869\n",
            "avarage training accuracy at epoch 22: 0.9733159406704847\n",
            "avarage loss: 0.31877774745225906\n",
            "avarage accuracy: 0.9235853637005649\n",
            "avarage training loss at epoch     23: 0.07296748415140783\n",
            "avarage training accuracy at epoch 23: 0.9740980294881249\n",
            "avarage loss: 0.3215794935822487\n",
            "avarage accuracy: 0.9235632944915255\n",
            "avarage training loss at epoch     24: 0.07352275365574212\n",
            "avarage training accuracy at epoch 24: 0.9740890711741648\n",
            "avarage loss: 0.32157369951407117\n",
            "avarage accuracy: 0.9236184675141246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qBar_CKfnCX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1989
        },
        "outputId": "46d87a03-79ea-43ef-e49f-14a90a31bd60"
      },
      "cell_type": "code",
      "source": [
        "trn_seq = batchSeq(X_trn, y_trn, bptt, bptt, 32, randlen=True)\n",
        "print(len(trn_seq))\n",
        "val_seq = batchSeq(X_val[:val_id//4], y_val[:val_id//4], bptt, bptt, 32, randlen=False)\n",
        "#model.load_state_dict(torch.load(\"my model.pt\"))\n",
        "%lprun -f fit losses, _ =  fit(model, 2e-4, trn_seq, val_seq=val_seq, epochs=1, wd=0, batches_per_epoch=len(trn_seq)//8)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9832\n",
            "number of batches:  9832\n",
            "avarage loss at i = 200: 0.0554526672384236\n",
            "avarage accuracy at i = 200: 0.9825204876844669\n",
            "avarage loss at i = 400: 0.0510484278074\n",
            "avarage accuracy at i = 400: 0.9829468391038791\n",
            "avarage loss at i = 600: 0.0550358807509474\n",
            "avarage accuracy at i = 600: 0.9820301764585523\n",
            "avarage loss at i = 800: 0.04965997311286628\n",
            "avarage accuracy at i = 800: 0.9834794170506824\n",
            "avarage loss at i = 1000: 0.05592431938741356\n",
            "avarage accuracy at i = 1000: 0.9816385109785796\n",
            "avarage loss at i = 1200: 0.0530246452218853\n",
            "avarage accuracy at i = 1200: 0.9827929116556521\n",
            "avarage loss: 0.06634443951637996\n",
            "avarage accuracy: 0.9787645417853742\n",
            "avarage loss at i = 1400: 0.05049484431190649\n",
            "avarage accuracy at i = 1400: 0.9833574252577977\n",
            "avarage loss at i = 1600: 0.05199368970934302\n",
            "avarage accuracy at i = 1600: 0.9829193972688631\n",
            "avarage loss at i = 1800: 0.05213765719905496\n",
            "avarage accuracy at i = 1800: 0.9829211678805376\n",
            "avarage loss at i = 2000: 0.05605135989375412\n",
            "avarage accuracy at i = 2000: 0.9813519541028195\n",
            "avarage loss at i = 2200: 0.051148579234723004\n",
            "avarage accuracy at i = 2200: 0.9830203606928996\n",
            "avarage loss at i = 2400: 0.05093937056604773\n",
            "avarage accuracy at i = 2400: 0.9830550506376676\n",
            "avarage loss: 0.06573818852074254\n",
            "avarage accuracy: 0.9788995726495726\n",
            "avarage loss at i = 2600: 0.05895787780405953\n",
            "avarage accuracy at i = 2600: 0.9811313161080857\n",
            "avarage loss at i = 2800: 0.05461621569935232\n",
            "avarage accuracy at i = 2800: 0.9822721561336099\n",
            "avarage loss at i = 3000: 0.05269809598277789\n",
            "avarage accuracy at i = 3000: 0.9824477134352888\n",
            "avarage loss at i = 3200: 0.04937178890395444\n",
            "avarage accuracy at i = 3200: 0.9837459870264508\n",
            "avarage loss at i = 3400: 0.053278346620500086\n",
            "avarage accuracy at i = 3400: 0.9821167009131974\n",
            "avarage loss at i = 3600: 0.0547382308691158\n",
            "avarage accuracy at i = 3600: 0.9821460941958989\n",
            "avarage loss: 0.0639094333787929\n",
            "avarage accuracy: 0.9794827279202282\n",
            "avarage loss at i = 3800: 0.05595129478257149\n",
            "avarage accuracy at i = 3800: 0.981310713281579\n",
            "avarage loss at i = 4000: 0.054794150205561894\n",
            "avarage accuracy at i = 4000: 0.9817335475716579\n",
            "avarage loss at i = 4200: 0.060016576622147114\n",
            "avarage accuracy at i = 4200: 0.9803525170416632\n",
            "avarage loss at i = 4400: 0.05517368204309605\n",
            "avarage accuracy at i = 4400: 0.9816191362348187\n",
            "avarage loss at i = 4600: 0.05315750858397223\n",
            "avarage accuracy at i = 4600: 0.9822472798124176\n",
            "avarage loss at i = 4800: 0.059522947942605244\n",
            "avarage accuracy at i = 4800: 0.9805172500887545\n",
            "avarage loss: 0.06298989251500096\n",
            "avarage accuracy: 0.979847756410256\n",
            "avarage loss at i = 5000: 0.050527835090178996\n",
            "avarage accuracy at i = 5000: 0.9832386390962057\n",
            "avarage loss at i = 5200: 0.052305670556525\n",
            "avarage accuracy at i = 5200: 0.982566065433142\n",
            "avarage loss at i = 5400: 0.05101997674209997\n",
            "avarage accuracy at i = 5400: 0.9832234950237985\n",
            "avarage loss at i = 5600: 0.05295601464109495\n",
            "avarage accuracy at i = 5600: 0.9825178783990958\n",
            "avarage loss at i = 5800: 0.04894975689123385\n",
            "avarage accuracy at i = 5800: 0.98407914072271\n",
            "avarage loss at i = 6000: 0.04982728690025397\n",
            "avarage accuracy at i = 6000: 0.9834000280466353\n",
            "avarage loss: 0.06170469395115844\n",
            "avarage accuracy: 0.9803196225071221\n",
            "avarage loss at i = 6200: 0.05446975981816649\n",
            "avarage accuracy at i = 6200: 0.9816476505849286\n",
            "avarage loss at i = 6400: 0.058796915036946305\n",
            "avarage accuracy at i = 6400: 0.9813855317489341\n",
            "avarage loss at i = 6600: 0.0515245467168279\n",
            "avarage accuracy at i = 6600: 0.983253137022042\n",
            "avarage loss at i = 6800: 0.054763545581445215\n",
            "avarage accuracy at i = 6800: 0.9818226860774055\n",
            "avarage loss at i = 7000: 0.053016376777086406\n",
            "avarage accuracy at i = 7000: 0.9823316315118369\n",
            "avarage loss at i = 7200: 0.05447106400039047\n",
            "avarage accuracy at i = 7200: 0.982001910728592\n",
            "avarage loss: 0.06026667346939062\n",
            "avarage accuracy: 0.980650522317189\n",
            "avarage loss at i = 7400: 0.05623368815984577\n",
            "avarage accuracy at i = 7400: 0.9815139578882913\n",
            "avarage loss at i = 7600: 0.052879046213347464\n",
            "avarage accuracy at i = 7600: 0.9825636197524712\n",
            "avarage loss at i = 7800: 0.0466290756536182\n",
            "avarage accuracy at i = 7800: 0.9843361008909888\n",
            "avarage loss at i = 8000: 0.05145790324124391\n",
            "avarage accuracy at i = 8000: 0.9829617038192243\n",
            "avarage loss at i = 8200: 0.05541782199870795\n",
            "avarage accuracy at i = 8200: 0.9820739326285666\n",
            "avarage loss at i = 8400: 0.05167711625806987\n",
            "avarage accuracy at i = 8400: 0.98307001623924\n",
            "avarage loss at i = 8600: 0.054016390601173046\n",
            "avarage accuracy at i = 8600: 0.9821951283144584\n",
            "avarage loss: 0.05911336886030529\n",
            "avarage accuracy: 0.9809932929724597\n",
            "avarage loss at i = 8800: 0.05169852654915303\n",
            "avarage accuracy at i = 8800: 0.9828806855311683\n",
            "avarage loss at i = 9000: 0.05604360515251756\n",
            "avarage accuracy at i = 9000: 0.9817650692688126\n",
            "avarage loss at i = 9200: 0.049292556467917165\n",
            "avarage accuracy at i = 9200: 0.9833701125209031\n",
            "avarage loss at i = 9400: 0.04362880028318614\n",
            "avarage accuracy at i = 9400: 0.985529872894242\n",
            "avarage loss at i = 9600: 0.054975491794757546\n",
            "avarage accuracy at i = 9600: 0.981921661461142\n",
            "avarage loss at i = 9800: 0.050437247128866144\n",
            "avarage accuracy at i = 9800: 0.9832778028588254\n",
            "avarage loss: 0.05790470062789542\n",
            "avarage accuracy: 0.9813390313390307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R4Gdt3KSDc55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "543d64b9-fd31-4a14-d031-6db9c4061dc4"
      },
      "cell_type": "code",
      "source": [
        "trn_seq = batchSeq(X_trn, y_trn, bptt, bptt, 32, randlen=True)\n",
        "print(len(trn_seq))\n",
        "val_seq = batchSeq(X_test[:val_id//4], y_test[:val_id//4], bptt, bptt, 32, randlen=False)\n",
        "#model.load_state_dict(torch.load(\"my model.pt\"))\n",
        "%lprun -f fit losses, _ =  fit(model, 3e-4, trn_seq, val_seq=val_seq, epochs=1, wd=0, batches_per_epoch=len(trn_seq)//8)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9832\n",
            "number of batches:  9832\n",
            "avarage loss at i = 200: 0.041403167634271085\n",
            "avarage accuracy at i = 200: 0.9861772033763448\n",
            "avarage loss at i = 400: 0.041576813798783405\n",
            "avarage accuracy at i = 400: 0.9862693766555322\n",
            "avarage loss at i = 600: 0.04075142173824133\n",
            "avarage accuracy at i = 600: 0.9863678831023514\n",
            "avarage loss at i = 800: 0.042827661731280386\n",
            "avarage accuracy at i = 800: 0.9854852155247452\n",
            "avarage loss at i = 1000: 0.04527867020107806\n",
            "avarage accuracy at i = 1000: 0.9848085640709724\n",
            "avarage loss at i = 1200: 0.04601411873823963\n",
            "avarage accuracy at i = 1200: 0.9844833683473566\n",
            "avarage loss: 0.05052742458338666\n",
            "avarage accuracy: 0.9841373315949586\n",
            "avarage loss at i = 1400: 0.04265186666569207\n",
            "avarage accuracy at i = 1400: 0.9856582533976124\n",
            "avarage loss at i = 1600: 0.04489049159456045\n",
            "avarage accuracy at i = 1600: 0.9847137012932233\n",
            "avarage loss at i = 1800: 0.04260679536171665\n",
            "avarage accuracy at i = 1800: 0.9855992055715345\n",
            "avarage loss at i = 2000: 0.0491508737008553\n",
            "avarage accuracy at i = 2000: 0.9838395099454366\n",
            "avarage loss at i = 2200: 0.04676912752911448\n",
            "avarage accuracy at i = 2200: 0.9843470485950456\n",
            "avarage loss at i = 2400: 0.04902094529476017\n",
            "avarage accuracy at i = 2400: 0.9833926531721587\n",
            "avarage loss: 0.050567478293744035\n",
            "avarage accuracy: 0.9841509126466756\n",
            "avarage loss at i = 2600: 0.044813192706787956\n",
            "avarage accuracy at i = 2600: 0.9846327973444563\n",
            "avarage loss at i = 2800: 0.05122686513837835\n",
            "avarage accuracy at i = 2800: 0.9825760684662777\n",
            "avarage loss at i = 3000: 0.049412289708852766\n",
            "avarage accuracy at i = 3000: 0.9833775553816645\n",
            "avarage loss at i = 3200: 0.05031414874247275\n",
            "avarage accuracy at i = 3200: 0.9830712757310731\n",
            "*** KeyboardInterrupt exception caught in code being profiled."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qXz-kyP1M6RB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1989
        },
        "outputId": "3d0b37b8-8cef-47bf-cc60-86ed6d449236"
      },
      "cell_type": "code",
      "source": [
        "trn_seq = batchSeq(X_trn, y_trn, bptt, bptt, 32, randlen=True)\n",
        "print(len(trn_seq))\n",
        "val_seq = batchSeq(X_test[:val_id//4], y_test[:val_id//4], bptt, bptt, 32, randlen=False)\n",
        "model.load_state_dict(torch.load(\"my model.pt\"))\n",
        "%lprun -f fit losses, _ =  fit(model, 3e-5, trn_seq, val_seq=val_seq, epochs=1, wd=0.0, batches_per_epoch=len(trn_seq)//8)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9832\n",
            "number of batches:  9832\n",
            "avarage loss at i = 200: 0.03928922806400806\n",
            "avarage accuracy at i = 200: 0.9867964793335835\n",
            "avarage loss at i = 400: 0.041206079733092336\n",
            "avarage accuracy at i = 400: 0.9860861114064587\n",
            "avarage loss at i = 600: 0.03808613482822693\n",
            "avarage accuracy at i = 600: 0.9870889891464061\n",
            "avarage loss at i = 800: 0.03799593140021898\n",
            "avarage accuracy at i = 800: 0.9872769272158424\n",
            "avarage loss at i = 1000: 0.03704193865181878\n",
            "avarage accuracy at i = 1000: 0.9878143787700986\n",
            "avarage loss at i = 1200: 0.0398629853145394\n",
            "avarage accuracy at i = 1200: 0.9867635236925478\n",
            "avarage loss: 0.1684224225243718\n",
            "avarage accuracy: 0.9519049688541212\n",
            "avarage loss at i = 1400: 0.041359861278906465\n",
            "avarage accuracy at i = 1400: 0.9860671728659017\n",
            "avarage loss at i = 1600: 0.04130492753778526\n",
            "avarage accuracy at i = 1600: 0.9865324915975258\n",
            "avarage loss at i = 1800: 0.0429946172167547\n",
            "avarage accuracy at i = 1800: 0.985887600377906\n",
            "avarage loss at i = 2000: 0.041932297544553875\n",
            "avarage accuracy at i = 2000: 0.9864641941954393\n",
            "avarage loss at i = 2200: 0.0359451488580089\n",
            "avarage accuracy at i = 2200: 0.9880047294410567\n",
            "avarage loss at i = 2400: 0.04040641428797244\n",
            "avarage accuracy at i = 2400: 0.9867882327399878\n",
            "avarage loss: 0.16739219371786612\n",
            "avarage accuracy: 0.9525628953595057\n",
            "avarage loss at i = 2600: 0.03804586003738222\n",
            "avarage accuracy at i = 2600: 0.9872369878969668\n",
            "avarage loss at i = 2800: 0.035404962599277494\n",
            "avarage accuracy at i = 2800: 0.9884307281564712\n",
            "avarage loss at i = 3000: 0.04022186567119206\n",
            "avarage accuracy at i = 3000: 0.9869135577880398\n",
            "avarage loss at i = 3200: 0.0378948157094419\n",
            "avarage accuracy at i = 3200: 0.9874359375014309\n",
            "avarage loss at i = 3400: 0.03899804744167341\n",
            "avarage accuracy at i = 3400: 0.9871135252310219\n",
            "avarage loss at i = 3600: 0.03733896600897424\n",
            "avarage accuracy at i = 3600: 0.9875701995434899\n",
            "avarage loss: 0.16753726059297613\n",
            "avarage accuracy: 0.9528118813076436\n",
            "avarage loss at i = 3800: 0.03724782972363755\n",
            "avarage accuracy at i = 3800: 0.9876879361047289\n",
            "avarage loss at i = 4000: 0.037229623767198065\n",
            "avarage accuracy at i = 4000: 0.9874038483218315\n",
            "avarage loss at i = 4200: 0.04160446167807095\n",
            "avarage accuracy at i = 4200: 0.9861559055674808\n",
            "avarage loss at i = 4400: 0.0370911728090141\n",
            "avarage accuracy at i = 4400: 0.9877990567428518\n",
            "avarage loss at i = 4600: 0.034865250408183786\n",
            "avarage accuracy at i = 4600: 0.9885168873135135\n",
            "avarage loss at i = 4800: 0.03713769077963661\n",
            "avarage accuracy at i = 4800: 0.9878185512951174\n",
            "avarage loss: 0.1674742385674735\n",
            "avarage accuracy: 0.9529054396639135\n",
            "avarage loss at i = 5000: 0.037521657546167264\n",
            "avarage accuracy at i = 5000: 0.9874650422359434\n",
            "avarage loss at i = 5200: 0.03953611075121444\n",
            "avarage accuracy at i = 5200: 0.9874645934198676\n",
            "avarage loss at i = 5400: 0.035415208663143856\n",
            "avarage accuracy at i = 5400: 0.9882207023448784\n",
            "avarage loss at i = 5600: 0.0367509241681546\n",
            "avarage accuracy at i = 5600: 0.987931288346584\n",
            "avarage loss at i = 5800: 0.03722729504806921\n",
            "avarage accuracy at i = 5800: 0.9877781903118408\n",
            "avarage loss at i = 6000: 0.035494609592715276\n",
            "avarage accuracy at i = 6000: 0.9882901895061355\n",
            "avarage loss: 0.16648497232806436\n",
            "avarage accuracy: 0.9533189072383988\n",
            "avarage loss at i = 6200: 0.03697113425703719\n",
            "avarage accuracy at i = 6200: 0.9878361482404903\n",
            "avarage loss at i = 6400: 0.03668312525143847\n",
            "avarage accuracy at i = 6400: 0.9879124613173154\n",
            "avarage loss at i = 6600: 0.03587493076796818\n",
            "avarage accuracy at i = 6600: 0.9879241373919324\n",
            "avarage loss at i = 6800: 0.03801862264052033\n",
            "avarage accuracy at i = 6800: 0.9875202872873114\n",
            "avarage loss at i = 7000: 0.03689086069935001\n",
            "avarage accuracy at i = 7000: 0.9879421237761653\n",
            "avarage loss at i = 7200: 0.037911159671493805\n",
            "avarage accuracy at i = 7200: 0.9874826988010451\n",
            "avarage loss: 0.16677039216253958\n",
            "avarage accuracy: 0.9535180959969104\n",
            "avarage loss at i = 7400: 0.03419980982333073\n",
            "avarage accuracy at i = 7400: 0.9887501307933622\n",
            "avarage loss at i = 7600: 0.03627965849940665\n",
            "avarage accuracy at i = 7600: 0.9880699809628415\n",
            "avarage loss at i = 7800: 0.03430114178223448\n",
            "avarage accuracy at i = 7800: 0.9889146720598929\n",
            "avarage loss at i = 8000: 0.03674620763631538\n",
            "avarage accuracy at i = 8000: 0.9879830047881972\n",
            "avarage loss at i = 8200: 0.03420971863777595\n",
            "avarage accuracy at i = 8200: 0.9885353298995212\n",
            "avarage loss at i = 8400: 0.0334333880309714\n",
            "avarage accuracy at i = 8400: 0.9887850264966167\n",
            "avarage loss at i = 8600: 0.03755213698925217\n",
            "avarage accuracy at i = 8600: 0.9876630667753311\n",
            "avarage loss: 0.16733691953334884\n",
            "avarage accuracy: 0.9534833888647456\n",
            "avarage loss at i = 8800: 0.03939009983154392\n",
            "avarage accuracy at i = 8800: 0.9874975949375618\n",
            "avarage loss at i = 9000: 0.03402582410373725\n",
            "avarage accuracy at i = 9000: 0.9888534829095366\n",
            "avarage loss at i = 9200: 0.03743154440475337\n",
            "avarage accuracy at i = 9200: 0.9875327319193308\n",
            "avarage loss at i = 9400: 0.04063026925548911\n",
            "avarage accuracy at i = 9400: 0.9868158941719628\n",
            "avarage loss at i = 9600: 0.0377584273298271\n",
            "avarage accuracy at i = 9600: 0.9876816213245815\n",
            "avarage loss at i = 9800: 0.03667263481533155\n",
            "avarage accuracy at i = 9800: 0.9879506153660188\n",
            "avarage loss: 0.16715392676325372\n",
            "avarage accuracy: 0.9535512941233285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gbXsIa6pKUbn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e8cbb838-fdec-4182-9c1a-04efa0419632"
      },
      "cell_type": "code",
      "source": [
        "def find_lr(model, seq):\n",
        "  model.cuda()\n",
        "  lr = 1e-8\n",
        "  i = 0\n",
        "  while lr<1e-2:\n",
        "    lr *= 2\n",
        "    bx, by = seq[0]\n",
        "    outputs = model(V(bx))\n",
        "    opt = optim.RMSprop(model.parameters(), lr)\n",
        "    target = V(by)\n",
        "    loss = nll_loss_seq(outputs, target)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(lr, loss.data.cpu().numpy()[0])\n",
        "    \n",
        "output_dim = N_points+1\n",
        "model = charModelBi(vocab_size, 128, 768, 3, output_dim, res=False, dropout=True).cuda()\n",
        "bptt = 60\n",
        "\n",
        "torch.save(model.state_dict(), \"my model.pt\")\n",
        "#trn_seq = batchSeq(X_trn, y_trn, bptt, bptt, 64, randlen=True)\n",
        "find_lr(model, trn_seq)\n",
        "model.load_state_dict(torch.load(\"my model.pt\"))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2e-08 2.5704327\n",
            "4e-08 2.5703752\n",
            "8e-08 2.5706985\n",
            "1.6e-07 2.5699944\n",
            "3.2e-07 2.5692663\n",
            "6.4e-07 2.568564\n",
            "1.28e-06 2.5660367\n",
            "2.56e-06 2.5610957\n",
            "5.12e-06 2.5525289\n",
            "1.024e-05 2.5341272\n",
            "2.048e-05 2.4936934\n",
            "4.096e-05 2.3889506\n",
            "8.192e-05 1.9278762\n",
            "0.00016384 6.0058374\n",
            "0.00032768 2.5563734\n",
            "0.00065536 3.9217575\n",
            "0.00131072 3.7425306\n",
            "0.00262144 4.0890765\n",
            "0.00524288 2.8857484\n",
            "0.01048576 19.41905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L9AxmwSi_UN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "8dee18f1-35cf-4137-9359-bf1f87a19c13"
      },
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), \"my model1.pt\")\n",
        "%lprun -f fit losses, _ =  fit(model, 1e-4, trn_seq, val_seq=val_seq, epochs=20, wd=0.0003, batches_per_epoch=len(trn_seq))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of batches:  6721\n",
            "avarage loss at i = 200: 0.06504710240289568\n",
            "avarage accuracy at i = 200: 0.9784239244972671\n",
            "avarage loss at i = 400: 0.06026203079149127\n",
            "avarage accuracy at i = 400: 0.9796864708241853\n",
            "avarage loss at i = 600: 0.06609165752772242\n",
            "avarage accuracy at i = 600: 0.9782166353861521\n",
            "avarage loss at i = 800: 0.06241172423120588\n",
            "avarage accuracy at i = 800: 0.9793155276583703\n",
            "avarage loss at i = 1000: 0.06496478077955543\n",
            "avarage accuracy at i = 1000: 0.9781961354726805\n",
            "avarage loss at i = 1200: 0.06069545143283903\n",
            "avarage accuracy at i = 1200: 0.9805051591632838\n",
            "avarage loss at i = 1400: 0.06183864720631391\n",
            "avarage accuracy at i = 1400: 0.9795414015754207\n",
            "avarage loss at i = 1600: 0.06440245644189417\n",
            "avarage accuracy at i = 1600: 0.9789727380983131\n",
            "avarage training loss at epoch 0: 0.0032342853564868873\n",
            "avarage training accuracy at epoch 0: 0.04597936797031332\n",
            "avarage validation loss at epoch 0: 0.06771225292856495\n",
            "avarage validation accuracy at epoch 0: 0.9780875651041667\n",
            "avarage loss at i = 1800: 0.04121604030951857\n",
            "avarage accuracy at i = 1800: 0.5916170909355644\n",
            "avarage loss at i = 2000: 0.0633443573396653\n",
            "avarage accuracy at i = 2000: 0.9788750364814124\n",
            "avarage loss at i = 2200: 0.06111819399520755\n",
            "avarage accuracy at i = 2200: 0.9798058402039099\n",
            "avarage loss at i = 2400: 0.06067880137823522\n",
            "avarage accuracy at i = 2400: 0.9804142946432802\n",
            "avarage loss at i = 2600: 0.0655896510835737\n",
            "avarage accuracy at i = 2600: 0.9782092728671989\n",
            "avarage loss at i = 2800: 0.05930986446328461\n",
            "avarage accuracy at i = 2800: 0.9806005586708164\n",
            "avarage loss at i = 3000: 0.061337430155836044\n",
            "avarage accuracy at i = 3000: 0.979583773097413\n",
            "avarage loss at i = 3200: 0.05850827996619046\n",
            "avarage accuracy at i = 3200: 0.9802650969967679\n",
            "avarage training loss at epoch 0: 0.0062472560221240635\n",
            "avarage training accuracy at epoch 0: 0.09258809020271518\n",
            "avarage validation loss at epoch 0: 0.06605487781149956\n",
            "avarage validation accuracy at epoch 0: 0.9787733289930559\n",
            "avarage loss at i = 3400: 0.013910581562668086\n",
            "avarage accuracy at i = 3400: 0.20047764582389466\n",
            "avarage loss at i = 3600: 0.06509883384220302\n",
            "avarage accuracy at i = 3600: 0.9784209710585295\n",
            "avarage loss at i = 3800: 0.06574885237962008\n",
            "avarage accuracy at i = 3800: 0.9782699526692953\n",
            "avarage loss at i = 4000: 0.058344329548999666\n",
            "avarage accuracy at i = 4000: 0.9804502444628673\n",
            "*** KeyboardInterrupt exception caught in code being profiled."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ynq9DY2cBSQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1bc3769c-9694-4b5c-f206-fe02c2ae04cc"
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, seq, n=0):\n",
        "  model.eval()\n",
        "  acc_loss, acc_acc = 0, 0\n",
        "  i = 0\n",
        "  for bx, by in seq:\n",
        "    i += 1\n",
        "    \n",
        "    #pdb.set_trace()\n",
        "    outputs = model(V(bx))\n",
        "    target = V(by)\n",
        "    outputs, target = outputs[:, n:-n-1].contiguous(), target[:, n:-n-1].contiguous()\n",
        "    loss = nll_loss_seq(outputs, target)\n",
        "    acc_loss += loss.data.cpu().numpy()[0]\n",
        "    acc = accuracy(outputs, target)\n",
        "    acc_acc += acc.data.cpu().numpy()[0]\n",
        "  print(\"avarage loss: {}\".format(acc_loss/i))\n",
        "  print(\"avarage accuracy: {}\".format(acc_acc/i))\n",
        "  \n",
        "# test_seq = batchSeq(X_test, y_test, bptt, bptt, 128, randlen=False)\n",
        "evaluate(model, val_seq, 10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avarage loss: 0.03316952538024999\n",
            "avarage accuracy: 0.9900010957703269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jfoJUIWE4vlK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6093f265-7976-45f7-ba19-1c76c6afb5fc"
      },
      "cell_type": "code",
      "source": [
        "test_seq = batchSeq(X_test, y_test, bptt, bptt, 32, randlen=False)\n",
        "evaluate(model, test_seq, istest=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avarage loss: 0.09702100249744172\n",
            "avarage accuracy: 0.9691199047703793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Exz5PJgWY2tv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8505873-3384-419f-8ff7-e63299a07fd5"
      },
      "cell_type": "code",
      "source": [
        "test_seq = batchSeq(X_test, y_test, bptt, bptt, 32, randlen=False)\n",
        "evaluate(model, test_seq)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avarage loss: 0.07682160334984087\n",
            "avarage accuracy: 0.9764578971345108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MN1yOEG9lrrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4a55230-cea8-4f59-dcbe-4891017e93b3"
      },
      "cell_type": "code",
      "source": [
        "test_seq = batchSeq(X_test, y_test, bptt, bptt, 32, randlen=False)\n",
        "evaluate(model, test_seq)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avarage loss: 0.08125722883374044\n",
            "avarage accuracy: 0.9758737426914711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cw7jq2auzL9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "55f98e14-b0d9-4125-eb87-4975503f570a"
      },
      "cell_type": "code",
      "source": [
        "test_seq = batchSeq(X_test, y_test, bptt, bptt, 32, randlen=False)\n",
        "evaluate(model, test_seq, 5)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6d75203045a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batchSeq' is not defined"
          ]
        }
      ]
    }
  ]
}